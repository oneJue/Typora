 

### 文章目录

- [一：什么是卷积运算（了解）](#_7)
- [二：从全连接层到卷积层](#_33)
- - [（1）解决空间不变性](#1_41)
  - [（2）解决参数爆炸-稀疏连接和权值共享](#2_92)
- [三：CNN中的图像卷积](#CNN_120)

**卷积层：卷积层是CNN中的核心层，CNN中大部分计算量就集中在卷积层，卷积层主要完成卷积运算，目的是对输入数据进行特征提取，其要点如下**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fdeddd55f926f4f64ac51293db4cb83b8.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

# 一：什么是卷积运算（了解）

**在数学中，卷积是通过两个函数 f f f和 w w w生成第三个函数 s s s的数学算子**。假设有一个函数 f \( t \) f\(t\) f\(t\)用来表示在时刻 t t t发生的某一新闻对股价的影响，且时刻 t t t的股价不仅被该新闻所影响，还会被之前每天发生的相关新闻所影响。显然，距离 t t t时刻越近的新闻与股价就越相关，这里使用加权函数 w \( t − a \) w\(t-a\) w\(t−a\)对新闻赋予不同权重，其中 a a a表示发生该新闻的时间点， t − a t-a t−a就表示该新闻发生时刻 a a a与当前时刻 t t t的时间间隔。

对于时刻 t t t，基于新闻的股价预测函数就可以定义为

s \( t \) = ∑ a = − ∞ + ∞ f \( a \) w \( t − a \) s\(t\)=\\sum\\limits\_\{a=-\\infty\}\^\{+\\infty\}f\(a\)w\(t-a\) s\(t\)\=a\=−∞∑+∞​f\(a\)w\(t−a\)

也即时刻 t t t的股价预测结果是 t t t时刻及以前所有新闻对骨架的影响加权求和的结果。将上例中的 f f f和 w w w做一般化后，求 s s s的这种运算就叫做卷积。卷积运算通常用星号表示，这样这样我们就得到了离散形式的卷积公式

s \( t \) = \( f ∗ w \) \( t \) = ∑ α = − ∞ + ∞ f \( a \) w \( t − a \) s\(t\)=\(f\*w\)\(t\)=\\mathop\{\}\\sum\_\{\\alpha=-\\infty\}\^\{+\\infty\}f\(a\)w\(t-a\) s\(t\)\=\(f∗w\)\(t\)\=α\=−∞∑+∞​f\(a\)w\(t−a\)  
且当 f f f和 w w w为 R R R上的可积函数时，便可以得到卷积公式的积分形式

**而在CNN中使用的卷积与少奶奶是一种相当简单的运算，在神经网络中，第一个参数函数 f f f叫做输入，第二个参数 w w w叫做核函数（卷积核）**

s \( t \) = \( f ∗ w \) \( t \) = ∫ − ∞ + ∞ f \( a \) w \( t − a \)   d a s\(t\)=\(f\*w\)\(t\)=\\int\_\{-\\infty\}\^\{+\\infty\} f\(a\)w\(t-a\) \\, da s\(t\)\=\(f∗w\)\(t\)\=∫−∞+∞​f\(a\)w\(t−a\)da

# 二：从全连接层到卷积层

**从全连接层到卷积层：卷积层的出现也不是突然的，从某种方面它可以看作是全连接层的拓展，目的还是为了解决全连接层的缺点**

- 图像空间信息丢失（不具备空间不变性）
- 参数爆炸

## （1）解决空间不变性

对于传统的全连接层，假设输入个数为 n n n，那么第 j j j神经元的输出应该为（暂时忽略激活函数）， W i j W\_\{ij\} Wij​是一个二维张量表示与第 i i i个输入相连的第 j j j个权重，此时隐藏神经元输出为

H j = ∑ i n W i j X i + B H\_\{j\}=\\mathop\{\}\\sum\_\{i\}\^\{n\}W\_\{i\}jX\_\{i\}+B Hj​\=i∑n​Wi​jXi​+B

而在图像数据中，输入的是矩阵，并不是向量，所以我们可以尝试把这个权重也改为矩阵。运用类比的思想，将 W i , j W\_\{i,j\} Wi,j​权重变形为一个四维张量 W i , j , k , l W\_\{i,j,k,l\} Wi,j,k,l​，表示与第 \( k , l \) \(k,l\) \(k,l\)号输入相连的第 \( i , j \) \(i,j\) \(i,j\)号权重，此时隐藏神经元输出为

H i j = ∑ k ∑ l W i , j , k , l X k , l + U i , j H\_\{ij\}=\\mathop\{\}\\sum\_\{k\}\^\{\}\\mathop\{\}\\sum\_\{l\}\^\{\}W\_\{i,j,k,l\}X\_\{k,l\}+U\_\{i,j\} Hij​\=k∑​l∑​Wi,j,k,l​Xk,l​+Ui,j​

- X k , l X\_\{k,l\} Xk,l​就是输入矩阵中，第 k k k行第 l l l列的输入元素

对上式进行改写，重新索引下标 \( k , l \) \(k,l\) \(k,l\)，使 k = i + a k=i+a k\=i+a, l = j + b l=j+b l\=j+b，此时

H i , j = ∑ a ∑ b V i , j , a , b X i + a , j + b + U i , j H\_\{i,j\}=\\mathop\{\}\\sum\_\{a\}\^\{\}\\mathop\{\}\\sum\_\{b\}\^\{\}V\_\{i,j,a,b\}X\_\{i+a,j+b\}+U\_\{i,j\} Hi,j​\=a∑​b∑​Vi,j,a,b​Xi+a,j+b​+Ui,j​

这表示，对于隐藏层中任意位置 \( i , j \) \(i,j\) \(i,j\)处的神经元输出 H i , j H\_\{i,j\} Hi,j​，可以通过在 X X X中以 \( i , j \) \(i,j\) \(i,j\)为中心对像素进行加权求和得到，加权使用的权重为 V i , j , a , b V\_\{i,j,a,b\} Vi,j,a,b​

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ff6774f7c36314416a3c40c83ac609417.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

**解决平移不变性：**

局部性是指 **我们只需要关注一个图片的局部的特征，不应该偏离到距 \( i , j \) \(i,j\) \(i,j\)太远的地方，这意味必须限制 a , b a,b a,b在某一范围之内**

H i , j = u + ∑ a ∑ b V a , b X i + a , j + b H\_\{i,j\}=u+\\mathop\{\}\\sum\_\{a\}\^\{\}\\mathop\{\}\\sum\_\{b\}\^\{\}V\_\{a,b\}X\_\{i+a,j+b\} Hi,j​\=u+a∑​b∑​Va,b​Xi+a,j+b​  
这便是**卷积**，我们是在**使用系数 V a , b V\_\{a,b\} Va,b​对位置 \( i , j \) \(i,j\) \(i,j\)附近的像素 \( i + a , i + b \) \(i+a, i+b\) \(i+a,i+b\)进行加权得到 H i , j H\_\{i,j\} Hi,j​**。并且 V a , b V\_\{a,b\} Va,b​的系数要少于 V a , b , i , j V\_\{a,b,i,j\} Va,b,i,j​，因为前者不再依赖于图像中的位置，这意味必须限制 a , b a,b a,b在某一范围之内

**解决局部性：**

局部性是指 **我们只需要关注一个图片的局部的特征，不应该偏离到距 \( i , j \) \(i,j\) \(i,j\)太远的地方，这意味必须限制 a , b a,b a,b在某一范围之内**

H i , j = ∑ a = − Δ Δ V a , b X i + a , j + b H\_\{i,j\}=\\mathop\{\}\\sum\_\{a=-\\Delta\}\^\{\\Delta\}V\_\{a,b\}X\_\{i+a,j+b\} Hi,j​\=a\=−Δ∑Δ​Va,b​Xi+a,j+b​  
上面便是我们熟知的卷积层，其中 **V V V被称之为卷积核或滤波器**，当然也可以直接简单地称之为该卷积层的权重，通常该权重是可以学习的参数

**卷积层具有平移不变性和局部性**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F6cc112b6f61347f38e9ba3c163f52f7c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

## （2）解决参数爆炸-稀疏连接和权值共享

- 卷积层参数量计算公式为  
  \( C i n × （ K × K ） + 1 \) ∗ C o u t \(C\_\{in\}×（K×K）+1\)\*C\_\{out\} \(Cin​×（K×K）+1\)∗Cout​

---

如下图，在DNN中，全连接层的每个神经元都会受到输入层中所有神经元的影响，且权值各不相同。**而卷积层中的神经元只受到感受野范围内输入层神经元的影响**，且权值固定，这些神经元共享参数

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F54dde7d2da6845f996996946b80cea85.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

以之前的手写数字识别案例为例，每个样本是一个28×28的灰度图像，对于DNN来说，假设它只有唯一的隐藏层并且隐藏层中的神经元数量为2048，那么输入层到隐藏层的参数个数为

28 × 28 × 2048 + 2048 = 1607680 28×28×2048+2048=1607680 28×28×2048+2048\=1607680

而对于卷积层来说，不同的神经元代表的不是不同的卷积核，而是作用于不同感受野的相同的卷积核，所以对于一个5×5的卷积核，按照上面的例子，该卷积层的神经元个数为

\( 28 − 5 + 1 \) × \( 28 − 5 + 1 \) = 576 \(28-5+1\)×\(28-5+1\)=576 \(28−5+1\)×\(28−5+1\)\=576

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F020b2d9506324ffe81d7a4231cb4c8b2.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

因此，基于权值共享，576个神经元共享这5×5=25个权值，也就是说，不管神经元的个数有多少，参数只有25个二，而且偏置也是共享的，也即该卷积层卷积核参数为26个。当然一个卷积核仅仅代表了一类类型特征的提取，加入在卷积层加入了99种特征，也是还需要99个卷积核，但带训练的参数也仅仅只有2600个，大大降低了训练代价

# 三：CNN中的图像卷积

以二维卷积为例，如下图，直观俩今，卷积就像一把刷子，把输入矩阵按照从左到右、从上至下的顺序刷了一遍，然后得到了一个新的图像矩阵（特征图）

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F4a7067d9e574449bbf9f043ef46a4d80.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)

具体计算时，只需要将**原输入矩阵的值和卷积核对应位置相乘再相加，即可得到特征图对应位置值**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fbed8fe66b8604671a2f42f0a0a2c9edb.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128888043)