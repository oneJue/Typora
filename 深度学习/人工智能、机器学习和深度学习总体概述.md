 

### 文章目录

- [本文说明](#_2)
- [一：人工智能需要的基础和涉及内容](#_18)
- [二：数学基础](#_29)
- - [（1）线性代数](#1_31)
  - [（2）概率论](#2_106)
  - [（3）数理统计](#3_233)
  - [（4）最优化方法](#4_277)
  - [（5）信息论](#5_322)
- [三：机器学习](#_385)
- - [（1）线性回归](#1_448)
  - - [①：简述](#_450)
    - [②：概述](#_456)
  - [（2）朴素贝叶斯](#2_503)
  - - [①：简述](#_505)
    - [②：概述](#_517)
  - [（3）逻辑回归](#3_547)
  - - [①：简述](#_549)
    - [②：概述](#_554)
  - [（4）决策树](#4_594)
  - - [①：简述](#_596)
    - [②：概述](#_602)
  - [（5）支持向量机](#5_644)
  - - [①：简述](#_646)
    - [②：概述](#_651)
  - [（6）集成学习](#6_706)
  - [（7）聚类](#7_745)
  - [（8）降维](#8_749)
- [四：人工神经网络](#_781)
- - [（1）神经元和感知器](#1_801)
  - [（2）多层感知器](#2_853)
  - [（3）径向基函数神经网络](#3_897)
  - [（4）自组织特征映射](#4_936)
  - [（5）模糊神经网络](#5_958)
- [五：深度学习](#_971)
- - [（1）深度前馈网络](#1_1003)
  - [（2）深度学习中的正则化](#2_1019)
  - [（3）深度学习中的优化](#3_1043)
  - [（4）自编码器](#4_1066)
  - [（5）深度信念网络](#5_1099)
  - [（6）卷积神经网络](#6_1125)
  - [（7）循环神经网络](#7_1173)

# 本文说明

**文本大部分内容来自极客时间课程【人工智能基础课】，在学习完之后感觉对整个人工智能领域研究内容有了比较清楚的认知，但这么课程有个缺点就是部分内容叙述过于枯燥，而且重点不清晰，关键是配图太少，所以这里总结如下，然后再加上相应的一些配图，便于大家了解。本文只是介绍一些核心概念，如果想要更深入更完善的学习，还请点击下方进入课程了解，这门课程确实不错**

- [极客时间-人工智能基础课（作者：王天一）](https://time.geekbang.org/column/intro/100003101?tab=intro)  
  ![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F28df1fa21a3844bab49947bd8bf82d16.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)  
  ![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fa109f555558f414ba0dee585c01d977c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**除此之外，本文参考文章有**

- [推文：图解最常用的10大机器学习算法！](https://mp.weixin.qq.com/s/-nSCV__SEeXm6ZebGaZTgw)

# 一：人工智能需要的基础和涉及内容

- **数学基础**：蕴含着处理智能问题的基本思想与方法，也是理解复杂算法的必备要素，没有数学人工智能将无从谈起。主要包括**线性代数、概率论、数理统计、最优化方法、信息论、符号逻辑等**
- **机器学习**：机器学习的作用是从数据中习得学习算法，进而解决实际问题。主要包括**线性回归、决策树、支持向量机、聚类等等**
- **人工神经网络**：是机器学习的一个分支，神经网络将认知科学引入机器学习，以模拟生物神经系统对真实世界的交互反应。主要包括**多层神经网络、前馈与反向传播、自组织神经网络等**
- **深度学习**：简而言之，深度学习就是包含多个中间层的神经网络。主要包括**深度前馈网络、深度学习中的正则化、自编码器等**
- **神经网络实例**：在深度学习框架下，一些神经网络已用于各种应用场景，并取得了不俗的效果。主要包括**卷积神经网络、循环神经网络、深度信念网络等**
- **深度学习之外的人工智能**：深度学习即有优点也有缺陷，其他方向的人工智能研究正是有益的补充。主要包括**马尔科夫随机场、迁移学习、集群智能**
- **人工智能应用**：主要包括**计算机视觉、语音识别、对话系统等**

# 二：数学基础

## （1）线性代数

**线性代数\(linear algebra\)：线性代数为我们提供了一种看待世界的抽象视角，万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。人类能够感知连续变化的大千世界，可计算机只能处理离散取值的二进制信息，因而来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，线性代数是用虚拟数字世界表示真实物理世界的工具。线性代数有如下几个核心概念**

- **标量\(scalar\)**：由单独的数 a a a构成的元素称之为标量，标量可以是整数、实数或复数等
- **向量\(vector\)**：如果将向量的所有标量都替换为相同规格的向量则得到向量
- **矩阵\(matrix\)**：多个标量 a 1 , a 2 , . . . , a n a\_\{1\},a\_\{2\},...,a\_\{n\} a1​,a2​,...,an​按一定顺序组成一个序列称之为矩阵（可以理解为魔方的一个面）
- **张量\(tensor\)**：如果将矩阵中的每个标量元素再替换为向量的话，得到的就是张量，张量就是高阶的矩阵（可以理解为一个魔方）

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fbd6588cab51b416fa7564c0e0efefcf0.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**使用范数\(norm\)和内积\(inner product\)来描述特定向量：在实际问题中，向量意义并不仅仅在于数字组合，更有可能是某些对象和某些行为的特征，所以范数和内积可以处理这些特征，进而提取出隐含关系**

- **范数**：是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值  
  ![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fee0243aac9324c948a742037d498e9c6.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

- **内积**：用于描述两个向量之间的关系，可以表示向量之间的相对位置，也即向量之间的夹角

  - **正交**：如果向量内积为0，则称两个向量正交（在二维空间中，表现为相互垂直）。正交意味着两个向量线性无关

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc97645bfea1142e3ab7afec3d74235f0.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**线性空间\(linear space\)**：如果有一个集合，它的元素都是**具有相同维数的向量**\(可以是有限个或无限个\)，并且定义了加法和数乘等结构化的运算,这样的集合就被称为线性空间\(linear space\)。在线性空间中，**任意一个向量代表的都是 n n n维空间中的一个点**；反之，**空间中的任意点也可以唯一用一个向量表示**

- **内积空间\(inner product space\)**：定义了内积运算的线性空间

**正交基\(orthogonal basis\)**：在内积空间中，一组**两两正交的向量**构成这个空间的正交基；加入正交基中的**基向量**的 l 2 l\^\{2\} l2范数都是单位长度1，这组正交基就称之为**标准正交基**。正交基的作用就是**给内积空间定义经纬度**，一旦描述内积空间的正交基确定了，**向量和点之间的对应关系也就随之确定了**

---

**用矩阵描述变化**：线性空间的一个重要特征是能够承载**变化**。当作为参考系的标准正交基确定后，**空间中的点就可以用向量表示**。**当这个点从一个位置移动到另一个位置时，描述它的向量也会发生改变。点的变化对应着向量的线性变换\(linear transformation\)**，而描述对象变化又或者向量变换的数学语言正是**矩阵。在线性空间中，变化的实现方式有两种**

- 直接对点本身进行变化
- 对参考系变化

**因此对于矩阵和向量相乘，就存在着不同的解读方式**

- 向量 x x x经过矩阵 A A A所描述的变换，变为了向量 y y y
- 一个对象在坐标系下 A A A的度量结果为 x x x，在标准坐标系 I I I下度量结果为 y y y

A X = y AX=y AX\=y

---

**特征值\(eigenvalue\)和特征向量\(eigenvector\)：对于给定矩阵 A A A，假设其特征值为 λ \\lambda λ，特征向量为 x x x，则它们之间的关系如下**

A x = λ x Ax=\\lambda x Ax\=λx

**特征值和特征向量的动态意义在于表示了变化的速度和方向。矩阵有多个特征值和特征向量的含义就是说矩阵会议不同的速度在不同的方向上变化，最终矩阵的变化是这些分变化叠加的效果**

## （2）概率论

**概率论\(probability theory\)：概率论也代表了一种看待世界的方式，它关注的焦点是无处不在的可能性，对随机事件发生的可能性进行规范的数学描述就是概率论公理化的过程**

**古典概型**：说到概率论，我们首先会想到抛硬币实验。将同一枚硬币抛掷10次，其正面朝上的次数可能一次没有，也可能全部都是，换算成**频率**则分别对应0\%和100\%。频率会随机波动，但随着重复实验次数不断增加，**会趋于某个常数，这种从事件发生的频率认识概率的方法称之为频率学派**，它们口中的概率其实**就是一个可独立重复的随机试验中单个结果出现频率的极限**。在概率的定量计算中，频率所依赖的基础时**古典概率模型**，在古典概率模型中，**试验的结果只包含有限个基本事件，且每个基本事件发生的可能性相同**。所以假设所有基本事件的数目为 n n n，待观察的随机事件 A A A中包含的基本事件数目为 k k k，则古典概率模型下事件概率的计算公式为

P \( A \) = k n P\(A\)=\\frac\{k\}\{n\} P\(A\)\=nk​

---

**条件概率（conditional probaility）：是根据已有信息对样本空间进行调整后得到的新的概率分布。假定有两个随机事件 A A A和 B B B，条件概率是指事件 A A A在事件 B B B已经发生的条件下发生的概率，即**

- **联合概率 P \( A B \) P\(AB\) P\(AB\)（joint probaility）**：表示的是 A A A和 B B B两个事件共同发生的概率，如果两个事件相互独立则有 P \( A B \) = P \( A \) P \( B \) P\(AB\)=P\(A\)P\(B\) P\(AB\)\=P\(A\)P\(B\)

P \( A ∣ B \) = P \( A B \) P \( B \) P\(A|B\)=\\frac\{P\(AB\)\}\{P\(B\)\} P\(A∣B\)\=P\(B\)P\(AB\)​

**全概率公式（law of total probability）：基于条件概率可得出全概率公式，全概率公式在于将复杂事件的概率求解转化为在不同情况下发生的简单事件的概率求和，也即**

P \( A \) = ∑ i = 1 N P \( A ∣ B i \) P \( B i \) , ∑ i = 1 N P \( B i \) = 1 P\(A\)=\\sum\\limits\_\{i=1\}\^\{N\}P\(A|B\_\{i\}\)P\(B\_\{i\}\),\\quad \\sum\\limits\_\{i=1\}\^\{N\}P\(B\_\{i\}\)=1 P\(A\)\=i\=1∑N​P\(A∣Bi​\)P\(Bi​\),i\=1∑N​P\(Bi​\)\=1

---

**贝叶斯公式：对全概率公式稍作整理，就演化除了求解“逆概率”这一问题，逆概率是指在事件结果已经确定的条件下 P \( A \) P\(A\) P\(A\)，推断出各种假设发生的可能性（ P \( B i ∣ A \) P\(B\_\{i\}|A\) P\(Bi​∣A\)）**

P \( B i ∣ A \) = P \( A ∣ B i \) P \( B i \) ∑ j = 1 N P \( A ∣ B j \) P \( B j \) P\(B\_\{i\}|A\)=\\frac\{P\(A|B\_\{i\}\)P\(B\_\{i\}\)\}\{\\sum\\limits\_\{j=1\}\^\{N\}P\(A|B\_\{j\}\)P\(B\_\{j\}\)\} P\(Bi​∣A\)\=j\=1∑N​P\(A∣Bj​\)P\(Bj​\)P\(A∣Bi​\)P\(Bi​\)​

**贝叶斯定理：贝叶斯公式可进一步抽象为贝叶斯定理。贝叶斯定理根据观测结果寻找合理的假设，或者说根据观测数据寻找最佳的理论解释，其关注焦点在于后验概率**

- **P \( H \) P\(H\) P\(H\)为先验概率**：预先设定的假设成立的概率
- **P \( D ∣ H \) P\(D|H\) P\(D∣H\)为似然概率**：在假设成立的前提下观测到结果的概率
- **P \( D ∣ H \) P\(D|H\) P\(D∣H\)为后验概率**：在观测到结果的前提下假设成立的概率

P \( H ∣ D \) = P \( D ∣ H \) P \( H \) P \( D \) P\(H|D\)=\\frac\{P\(D|H\)P\(H\)\}\{P\(D\)\} P\(H∣D\)\=P\(D\)P\(D∣H\)P\(H\)​

**概率论的贝叶斯学派认为概率描述的是随机事件的可信程度。例如明天下雨的概率时85\%指的是明天下雨这个事件的可信度为85\%**

---

**频率学派和贝叶斯学派区别**

- **频率学派**：认为**假设是客观存在的且不会改变的，也即存在固定的先验分布，只是作为观察者的我们无从知晓**。因而在计算具体事件的概率时要先确定概率分布的类型和参数，以此为基础进行概率演算
- **贝叶斯学派**：认为**固定的先验分布是不存在的，参数本身也是随机数**。换言之，假设本身取决于观察结果，是不确定并且可以修正的。数据的作用就是对假设做出不断的修正，使观察者对概率的主观认识更加接近客观实际

---

**概率估计方法：机器学习中很多模型都会采用概率论的方法，但由于实际任务中可供使用的训练数据有限，所以需要对概率分布的参数进行估计，这也是机器学习的核心任务。主要有两种估计方法**

- **最大似然估计（在机器学习中更为重要）**：使训练数据出现的概率最大化，依次确定概率分布中的未知参数，估计出的概率分布也就最符号训练数据的分布。最大似然估计只需要训练数据
- **最大后验概率法**：根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最可能的未知参数取值作为估计值。最大后验概率法需要先验概率

---

**随机变量：根据取值空间的不同，随机变量可分为**

- **离散型随机变量**
- **连续性随机变量**

**随机变量：根据取值空间的不同，随机变量可分为**

- **离散型随机变量**
- **连续性随机变量**

**概率质量函数和概率密度函数：根据取值空间的不同，随机变量可分为**

- **概率质量函数**：离散型随机变量每个可能的取值都具有大于0的概率，取值和概率之间对应的关系就是离散型随机变量的分布律，也即概率质量函数
- **概率密度函数**：概率质量函数在连续性随机变量上的对应就是概率密度函数（概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性  
  之间的相对关系）

---

**重要的离散分布**

- **两点分布**：适用于随机试验的结果是二进制的情形，事件发生、不发生的概率分别为 p p p和 1 − p 1-p 1−p 。任何只有两个结果的随机试验都可以用两点分布描述（例如抛掷硬币）
- **二项分布**：将满足参数为 p p p的两点分布的随机试验独立重复 n n n次，事件发生的次数即满足参数为 \( n , p \) \(n,p\) \(n,p\)的二项分布。二项分布表达式为 P \( X = k \) = C k n p k \( 1 − p \) n − k , 0 ≤ k ≤ n P\(X=k\)=C\_\{k\}\^\{n\}p\^\{k\}\(1-p\)\^\{n-k\},0\\leq k\\leq n P\(X\=k\)\=Ckn​pk\(1−p\)n−k,0≤k≤n
- **泊松分布**：放射性物质在规定时间内释放出的粒子数所满足的分布，参数为 λ \\lambda λ的泊松分布表达式为 P \( X = k \) = λ k k \! e − λ P\(X=k\)=\\frac\{\\lambda\^\{k\}\}\{k\!\}e\^\{-\\lambda\} P\(X\=k\)\=k\!λk​e−λ

**重要的连续分布**

- **均匀分布**：在区间 \( a , b \) \(a, b\) \(a,b\)上满足均匀分布的连续型随机变量，其概率密度函数为 1 b − a \\frac\{1\}\{b-a\} b−a1​，这个变量落在区间 \( a , b \) \(a,b\) \(a,b\)内任意等长度的子区间内的可能性是相同的

- **指数分布**：满足参数为 θ \\theta θ指数分布的随机变量只能取正值，其概率密度函数为 e − x θ / θ , x > 0 e\^\{\\frac\{-x\}\{\\theta\}\}/\\theta,x > 0 eθ−x​/θ,x\>0。指数分布的一个重要特性是无记忆性，也即 P \( X > s + t ∣ X > s \) = P \( X > t \) P\(X>s+t|X>s\)=P\(X>t\) P\(X\>s+t∣X\>s\)\=P\(X\>t\)

- **正态分布**：参数为正态分布的概率密度函数为 f \( x \) = 1 2 π σ e − \( x − u \) 2 2 σ 2 f\(x\)=\\frac\{1\}\{\\sqrt\{2\\pi\\sigma\}\}e\^\{-\\frac\{\(x-u\)\^\{2\}\}\{2\\sigma\{2\}\}\} f\(x\)\=2πσ ​1​e−2σ2\(x−u\)2​

  - 当 u = 0 , σ = 1 u=0,\\sigma=1 u\=0,σ\=1时，即 f \( x \) = 1 2 π e − x 2 2 f\(x\)=\\frac\{1\}\{\\sqrt\{2\\pi\}\}e\^\{-\\frac\{x\^\{2\}\}\{2\}\} f\(x\)\=2π ​1​e−2x2​为标准正态分布

---

**数字特征**

- **数学期望**：也即均值，体现的是随机变量可能取值的加权平均，也即根据每个取值出现的概率描述作为一个整体的随机变量的规律
- **方差**：是随机变量的取值与数学期望的偏离程度
- **协方差**：度量了两个随机变量之间的线性相关性，也即变量 y y y能否表示成以另一变量 x x x为自变量的 a x + b ax+b ax+b形式

## （3）数理统计

**数理统计（mathematical statistics））：根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断。你可以简单的理解为（当然不准确）“数理统计可以看成是逆向的概率论”**

- **概率论**：前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征和规律
- **数理统计**：是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据观察结果对原始分布做出推断

**数理统计有以下两个核心概念**

- **样本（sample）**：表示可用资源的有限数据集合
- **总体（population）**：观察对象所有的可能取值

**因此，数理统计的任务就是根据样本推断总体的数字特征。统计推断的基本问题可以分为参数估计和假设检验**

---

**参数估计：通过随机抽取的样本来估计总体分布的方法，可分为**

- **点估计**
- **区间估计**

---

**点估计：已知总体分布函数形式，但未知其一个或多个参数时，借助于总体的一个样本来估计未知参数的取值，主要包括两种方法**

- **矩估计法**：矩表示的是随机变量的分布特征， k k k阶矩的定义为随机变量的 k k k次方的均值，也即 E \( X k \) E\(X\^\{k\}\) E\(Xk\)，其思想在于用样本的 k k k阶矩估计总体的 k k k阶矩
- **最大似然估计法**：既然抽样得到的是已有的样本值，就可以认为取到这一组样本值的概率较大，因而在估计参数 θ \\theta θ的时候就需要让已有样本值出现的可能性最大.在最大似然估计中，似然函数被定义为样本观测值出现的概率，确定未知参数的准则是让似然函数的取值最大化，也就是微积分中求解函数最大值的问题

**区间估计：在估计未知参数 θ \\theta θ的过程中，除了求出估计量，还需要估计出一个区间，并且确定这个区间包含 θ \\theta θ真实值的可信程度。在数理统计中，这个区间被称为置信区间，这种估计方式则被称为区间估计**

---

**假设检验：参数估计的对象是总体的某个参数，假设检验的对象则是关于总体的某个论断，即关于总体的假设。假设检验中的假设包含原假设 H 0 H\_\{0\} H0​和备用假设 H 1 H\_\{1\} H1​，检验的过程就是根据样本在 H 0 H\_\{0\} H0​和 H 1 H\_\{1\} H1​之间选择一个接受的过程。假设检验的作用就在于根据学习器在测试集上的性能推断其泛化能力的强弱，并确定所得结论的精确程度，可以进一步推广为比较不同学习器的性能**

## （4）最优化方法

**最优化方法（optimization）：从本质上来讲，人工智能的目标就是最优化，在复杂环境与多体交互中做出最优决策。最优化理论研究的是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值（最小值）的数值。根据约束条件的不同，最优化问题可以分为无约束优化和约束优化两类**

- 线性规划是一类典型的约束优化问题，其解决的问题通常是在有限  
  的成本约束下取得最大的收益。但通过引入**拉格朗日乘子**可将含有 n n n个变量和 k k k个约束条件的问题转换为含有 \( n + k \) \(n+k\) \(n+k\)个变量的无约束优化问题

---

**梯度下降法：求解无约束优化问题最常用的方法是梯度下降法。其基本思想就是沿着目标函数值下降最快的方向寻找最小值，在数学上，梯度的方向是目标函数导数的反方向。对于多元函数来说，其沿负梯度方向下降最快**

- **步长**：是梯度下降算法中的一个重要影响因素，它是指每次更新 f \( x \) f\(x\) f\(x\)时 x x x的变化值。较小步长会导致收敛较慢，太大容易导致 f \( x \) f\(x\) f\(x\)越过最小值点

**梯度下降法只用到了目标函数的一阶导数，并未使用到二阶导数**

- 一阶导数描述的是目标函数如何随输入的变化而变化
- 二阶导数描述的则是一阶导数如何随输入的变化而变化，通过了关于目标函数**曲率**的信息

**曲率会影响目标函数下降速度**

- 曲率为正，目标函数会比梯度下降法的预期下降得更慢
- 曲率为负，目标函数则会比梯度下降法的预期下降得更快

**牛顿法：将二阶导数引入优化过程，得到的典型方法就是牛顿法.在牛顿法中，目标函数首先被泰勒展开，写成二阶近似的形式（相比之下，梯度下降法只保留了目标函数的一阶近似）。此时再对二阶近似后的目标函数求导，并令其导数等于 0，得到的向量表示的就是下降最快的方向。相比于梯度下降法，牛顿法的收敛速度更快**

---

**启发式算法：核心思想就是大自然中 " 优胜劣汰 " 的生存法则，并在算法的实现中添加了选择和突变等经验因素，例如**

- 模拟生物进化规律的遗传算法
- 模拟统计物理中固体结晶过程中的退火算法
- 模拟低等动物产生集群智能的蚁群算法

**神经网络本质也是一种启发式算法，它模拟的则是大脑中神经元竞争和协作的机制**

## （5）信息论

**信息论：不确定性才是客观世界的本质属性，不确定性只能用概率模型来描述，正是对概率的刻画促成了信息论的诞生。信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。"熵"这一概念已经在热力学中的到了广泛使用，但知道引申到信息论后，熵的本质才被解释清楚，即一个系统内在的混乱程度**

**在信息论中，如果事件 A A A发生的概率为 p \( A \) p\(A\) p\(A\)，则这个事件的自信息量定义为**

h \( A \) = − l o g 2 p \( A \) h\(A\)=-log\_\{2\}p\(A\) h\(A\)\=−log2​p\(A\)

**举个例子**：如果国足闯进世界杯决赛圈，1:1000 的夺冠赔率是个很乐观的估计，用这个赔率计算出的信息量约为 10 比特；而国乒夺冠的赔率不妨设为 1:2，即使在这样高的赔率下，事件的信息量也只有 1 比特。两者之间的差距正是其可能性相差悬殊的体现

---

**信息熵：根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。信源的信息熵是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计平均值。如果一个离散信源 X X X包含 n n n个符号，每个符号 a i a\_\{i\} ai​的取值为 p \( a i \) p\(a\_\{i\}\) p\(ai​\)，则 X X X的信息熵为**

H \( X \) = − ∑ i = 1 n p \( a i \) l o g 2 p \( a i \) H\(X\)=-\\sum\\limits\_\{i=1\}\^\{n\}p\(a\_\{i\}\)log\_\{2\}p\(a\_\{i\}\) H\(X\)\=−i\=1∑n​p\(ai​\)log2​p\(ai​\)

**信源熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。当信源中的每个符号的取值概率相等时，信源熵取到最大值 l o g 2 n log\_\{2\}n log2​n，意味着信源的随机程度最高**

---

**条件熵：如果两个信源之间具有相关性，那么在已知其中一个信源 X X X的条件下，另一个信源 Y Y Y的信源熵就会减小。条件熵 H \( Y ∣ X \) H\(Y|X\) H\(Y∣X\)表示的是在已知随机变量 X X X的条件下另一随机变量 Y Y Y的不确定性，也就是在给定 X X X时，根据 Y Y Y的条件概率计算出的熵再对 X X X求解数学期望**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ff54f91181ddd4f9cb8b02c70b20a52a4.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)  
**条件熵的意义在于先按照变量 X X X的取值对变量 Y Y Y进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照 X X X的分布计算其数学期望**

---

**互信息：互信息等于 Y Y Y的信源熵减去已知 X X X时 Y Y Y的条件熵，即由 X X X提供的关于 Y Y Y的不确定性的消除，也可以看成是 X X X给 Y Y Y带来的信息增益**

**在机器学习中，互信息常常被用于分类特征的选择**：对于给定的训练数据集 Y Y Y， H \( Y \) H\(Y\) H\(Y\)表示在未给定任何特征时，对训练集进行分类的不确定性； H \( Y ∣ X \) H\(Y|X\) H\(Y∣X\)则表示了使用特征 X X X对训练集 Y Y Y分类不确定性的减少程度，也就是特征 X X X对训练集 Y Y Y的区分度

---

**KL散度：描述两个概率分布 P P P和 Q Q Q之间的差异的一种方法，其定义为**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fa21a9273fcda41458858f985d3dc901b.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**KL 散度是对额外信息量的衡量**。给定一个信源，其符号的概率分布为 P \( X \) P\(X\) P\(X\) ，就可以设计一种针对 P \( X \) P\(X\) P\(X\)的最优编码，使得表示该信源所需的平均比特数最少（等于该信源的信源熵）

**KL 散度有如下两个重要性质**

- **非负性**：是指 KL 散度是大于或等于 0 的，等号只在两个分布完全相同时取到
- **非对称性**：KL 散度并不满足数学意义上对距离的定义，也即 D K L \( P ∣ ∣ Q \) ≠ D K L \( Q ∣ ∣ P \) D\_\{KL\}\(P||Q\)\\neq D\_\{KL\}\(Q||P\) DKL​\(P∣∣Q\)\=DKL​\(Q∣∣P\)

---

**最大熵原理：是确定随机变量统计特性时力图最符合客观情况的一种准则。对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值。这个时候的概率分布最均匀，也就是随机变量的随机程度最高，对它进行预测也就最困难**

**最大熵模型：将最大熵原理应用到分类问题上就可以得到最大熵模型。在分类问题中，首先要确定若干特征函数作为分类的依据。为了保证特征函数的有效性，其在模型真实分布 P \( X \) P\(X\) P\(X\)上的数学期望和在由训练数据集推导出的经验分布 P   \( x \) \\mathop\{P\}\\limits\^\{\~\}\(x\) P​\(x\)上的数学期望应该相等，即对给定特征函数数学期望的估计应该是个无偏估计量**

# 三：机器学习

人的大脑具有非常强的学习能力，**能从大量现象中提取反复出现额规律和模式**，比如如果你经常接触中日韩三个国家的人，你会很容易分辨出他们，即便他们很相似

- 中国人下颌适中
- 日本人长脸长
- 韩国人眼小颧高

**机器学习：人类的学习机制在人工智能中的实现便是机器学习。从方法论角度来看，机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科。假设已有数据具有一定的统计特性，则不同的数据可以视为满足独立同分布的样本。机器学习要做的就是根据已有的训练数据推导出描述所有数据的模型，并根据得出的模型实现对未知的测试数据的最优预测**

---

**机器学习中的数据**：在机器学习中，数据并非通常意义上的数量值，而是对于对象某些性质的描述。被描述的性质叫作**属性**，属性的取值称为**属性值**，不同的属性值有序排列得到的**向量**就是数据，也叫**实例**

- **例如**：用“肤色、眼睛大小、鼻子长短、颧骨高低”来描述人的面相特点，那么中国人普遍就是\{浅，大，短，低\}，而韩国人普遍就是\{浅，小，长，高\}

注意**特征空间**和**特征向量**

- **特征空间**：数据的不同属性之间可以视为相互独立，因而每个属性都代表了一个不同的维度，这些维度共同张成了特征空间
- **特征向量**：每一组属性值的集合都是这个空间中的一个点，因而每个实例都可以视为特征空间中的一个向量，即特征向量

---

**误差：在机器学习中，误差被定义为学习器的实际预测输出与样本真实输出之间的差异。就像在实际生活中，我们也很容易把一个韩国人认作中国人。误差又可以继续分为训练误差和测试误差**

- **训练误差**：指的是学习器在训练数据集上的误差，也称**经验误差**；描述的是输入属性与输出分类之间的相关性，能够**判定给定的问题是不是一个容易学习的问题**
- **测试误差**：指的是学习器在新样本上的误差，也称泛化误差；反映了学习器对未知的测试数据集的预测能力，是机器学习中的重要概念

---

**过拟合：由于是学习时模型包含的参数过多，从而导致训练误差较低但测试误差较高；过拟合无法避免，只能降低其影响**

- **例如**：如果你接触的韩国人较少，你可能会错误的认为韩国人都是单眼皮

**欠拟合：也即学习能力太弱，以致于训练数据的基本性质都没能学到；欠拟合可以通过改进学习器的算法克服**

- **例如**：学习能力太多，导致把黑猩猩的图像误认为是人

---

**交叉验证：在模型选择中，为了对测试误差做出更加精确的估计，一种广泛使用的方法是交叉验证。交叉验证思想在于重复利用有限的训练样本，通过将数据切分成若干子集，让不同的子集分别组成训练集与测试集，并在此基础上反复进行训练、测试和模型选择，达到最优效果**

---

**机器学习分类：机器学习的任务分成以下三类**

- **监督学习**：基于已知类别的训练数据进行学习
- **无监督学习**：基于未知类别的训练数据进行学习
- **半监督学习**：同时使用已知类别和未知类别的训练数据进行学习

## （1）线性回归

### ①：简述

**线性回归：线性回归用一个等式表示，通过找到输入变量的特定权重（ w w w），来描述输入变量（ x x x）与输出变量（ y y y）之间的线性关系**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fa444b36d8c16478f8ae70d74910b89fb.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

### ②：概述

**线性回归：线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数。具体来说，线性回归的作用是求得一组参数 w i , i = 0.1. , , , . n w\_\{i\},i=0.1.,,,.n wi​,i\=0.1.,,,.n，使预测输出可以表示为以这组参数为权重的实例属性的线性组合，引入常量 x 0 = 1 x\_\{0\}=1 x0​\=1，线性回归试图学习的模型就是**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F73567af1b2794cf7accf088ec04ffbbb.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**当实例只有一个属性时，输入和输出之间的关系就是二维平面上的一条直线；当实例的属性数目较多时，线性回归得到的就是 n 维空间上的一个超平面，对应一个维度等于 n \- 1 的线性子空间**

---

**最小二乘法：在线性回归中是以均方误差来度量预测输出 f \( x \) f\(x\) f\(x\)和真实输出 y y y之间的差异大小的。而以使均方误差取最小值为目标的模型求解方法就是最小二乘法**

- 在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使所有样本到直线的欧式距离之和最小
- 假定影响样本点的噪声满足参数为 \( 0 , σ 2 \) \(0,\\sigma\^\{2\}\) \(0,σ2\)的正态分布，在这种情形下，对参数 w w w的推导就可以用**最大似然**的方式进行。对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F63e651fba4e04a3bbc5249e878797207.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**确定了最小二乘法的最优性，接下来的问题就是如何求解均方误差的最小值。在单变量线性回归中，其回归方程可以写成 y = w 1 x + w 0 y=w\_\{1\}x+w\_\{0\} y\=w1​x+w0​。对 w 1 w\_\{1\} w1​和 w 2 w\_\{2\} w2​求偏导数，令两个偏导数均等于 0 的取值就是线性回归的最优解，其解析式可以写成**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fd338f2e4a16b4444a88a6a788fe7251b.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**多元线性回归：单变量线性回归是线性回归中最简单的一个例子，当样本的描述涉及多个属性时，这类问题被称为多元线性回归。多元线性回归中的参数 w w w也可以用最小二乘法进行估计，其最优解同样用偏导数确定。但参与运算的元素从向量变成了矩阵。在理想的情况下，多元线性回归的最优参数为**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F885cb62ec69b48c884ae7ed202347819.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**正则化：对于线性回归来说，无论采取怎样的选取标准，存在多个最优解都是不争的事实，这也意味着过拟合的产生。所以为了解决过拟合，常见的做法是正则化，即添加额外的惩罚项。常见的是岭回归和LASSO回归**

---

**线性回归与神经网络关系：不光是线性回归，多项式回归，逻辑回归等等也都可以看成是一个神经网络，例如可以把逻辑回归看做是仅含有一层神经元的单层的神经网络。一般用于二分类网络**

## （2）朴素贝叶斯

### ①：简述

**朴素贝叶斯：是一种简单但极为强大的预测建模算法，该模型由两种类型的概率组成，可以直接从你的训练数据中计算出来**

- 每个类别的概率
- 给定的每个 x x x值的类别的条件概率

**一旦计算出来，概率模型就可以用于使用贝叶斯定理对新数据进行预测**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F001015432e594a509aaacdddec213d52.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

### ②：概述

**朴素贝叶斯：朴素贝叶斯用于解决分类问题。其基本思想在于分析分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出**

**用 n n n维随机向量 x x x表示训练数据的属性，用随机变量 y y y表示分类结果，那么 x x x和 y y y的统计规律就可以联合概率分布 P \( X , Y \) P\(X,Y\) P\(X,Y\)描述，每一个具体的样本 \( x i , y i \) \(x\_\{i\},y\_\{i\}\) \(xi​,yi​\)都可以通过 P \( X , Y \) P\(X,Y\) P\(X,Y\)独立同分布地产生。朴素贝叶斯分类器的出发点就是这个联合概率分布，根据条件概率的性质可以得到**

P \( X , Y \) = P \( Y \) P \( X ∣ Y \) = P \( X \) P \( Y ∣ X \) P\(X,Y\)=P\(Y\)P\(X|Y\)=P\(X\)P\(Y|X\) P\(X,Y\)\=P\(Y\)P\(X∣Y\)\=P\(X\)P\(Y∣X\)

- **类先验概率 P \( Y \) P\(Y\) P\(Y\)**：代表每个类别出现的概率；容易计算，只要统计不同类别样本的数目即可
- **类似然概率 P \( X ∣ Y \) P\(X|Y\) P\(X∣Y\)**：代表在给定类别下不同属性出现的概率；受属性取值数目影响，估计较为困难

**要解决似然概率难以估计的问题，就需要“条件独立性假设”，它保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用。这样类条件概率就变成了属性条件概率的乘积**

- 属性的条件独立性假设是一个非常强的假设，有时会导致数据过于简化

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F3b04dbe5b3d34f09a032967acb99cdfa.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**有了训练数据集，先验概率 P \( Y \) P\(Y\) P\(Y\)和似然概率 P \( X ∣ Y \) P\(X|Y\) P\(X∣Y\)就可以视为已知条件，用来求解后验概率 P \( Y ∣ X \) P\(Y|X\) P\(Y∣X\)。对于给定的输入 x x x ，朴素贝叶斯分类器利用贝叶斯定理求解后验概率，并将后验概率最大的类作为输出。将属性条件独立性假设应用于后验概率求解中，就可以得到朴素贝叶斯分类器的数学表达式**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F0492ad00c7ed4889a72b70b20ce5859c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**拉普拉斯平滑：受训练数据集规模的限制，某些属性的取值在训练集中可能从未与某个类同时出现，这就可能导致属性条件概率为 0，此时直接使用朴素贝叶斯分类就会导致错误的结论。因为训练集样本的不充分导致分类错误，显然不是理想的结果。为了避免属性携带的信息被训练集中未曾出现过的属性值所干扰，在计算属性条件概率时需要添加一个称为“拉普拉斯平滑”的步骤**

## （3）逻辑回归

### ①：简述

**逻辑回归：逻辑回归与线性回归类似，这是因为两者的目标都是找出每个输入变量的权重值。 与线性回归不同的是，输出的预测值得使用称为逻辑函数的非线性函数进行变换，是二分类问题的专用方法**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F7411475aef004af7b592a0cfd933781c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

### ②：概述

**逻辑回归：逻辑回归本质是对线性回归的一种改进。引入单调可微函数 g \( ▪ \) g\(▪\) g\(▪\)，线性回归模型可推广为 y = g − 1 \( w T x \) y=g\^\{-1\}\(w\^\{T\}x\) y\=g−1\(wTx\)，进而将线性回归模型的连续预测值与分类任务的离散标记联系起来。当 g \( ▪ \) g\(▪\) g\(▪\)取成对数函数的形式时，线性回归就演变为了逻辑回归**

**在最简单的二分类问题中，分类的标记可以抽象为 0 和 1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑回归中，实现这一映射是对数几率函数，也叫 Sigmoid 函数**  
![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F916835078b72495f80bcc873de6be197.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**归根结底，逻辑回归模型由条件概率分布表示。对于给定的实例，逻辑回归模型比较两个条件概率值的大小，并将实例划分到概率较大的分类之中**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F630fe0ebfca44afdbc17a7afe368550e.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**学习时，逻辑回归模型在给定的训练数据集上应用最大似然估计法确定模型参数。对给定的数据集 \( x i , y i \) \(x\_\{i\},y\_\{i\}\) \(xi​,yi​\)，逻辑回归使每个样本属于其真实标记的概率最大化，以此为依据确定 w w w的最优值。似然函数可表示为**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F8f7bb938f2e44c9489afb1cd2c1170f1.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**对数似然函数为**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Feba084700b7a44599d6d164d3ca585d1.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**由于单个样本的标记 y i y\_\{i\} yi​只能取得 0 或 1，因而上式中的两项中只有一个有非零的取值。将每个条件概率的对数几率函数形式代入上式，经过化简可以得到**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F619e59378b4049e994356d32c47e2183.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解**

## （4）决策树

### ①：简述

**决策树：决策树模型可用二叉树表示，每个节点代表单个输入变量（x）和该变量上的左右孩子（假定变量是数字），树的叶节点包含用于进行预测的输出变量（y）**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F55c2ad41a87849b8b7648328dd35f911.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

### ②：概述

**决策树：决策树算法是解决分类问题的另一种方法，它采用用树形结构，然后层层推理来实现最终的分类。与朴素贝叶斯算法相比，决策树优势在于无需使用任何先验条件。决策树是一个包含根节点、内部节点和叶节点的树结构，其根节点包含样本全集，内部节点对应特征属性测试，叶节点则代表决策结果。以购房为例**

- 业主对每个可选房源都要做出 **“买”与“不买”** 的决策结果
- “每平米价格”、“房屋面积”、“学区房”等因素共同构成了**决策的判断条件**，在每个判断条件下的选择表示的是不同情况下的决策路径

**决策树模型的学习过程包括三个步骤：特征选择、决策树生成和决策树剪枝**

---

**特征选择：特征选择决定了使用哪些特征来划分特征空间。理想的特征选择是在每次划分之后，分支节点所包含的样本都尽可能属于同一个类别。在特征选择中通常使用的准则是信息增益（也即互信息）它描述的是在已知特征后对数据分类不确定性的减少程度，因而特征的信息增益越大，得到的分类结果的不确定度越低，特征也就具有越强的分类能力**

**对于信息增益，可用银行放贷款的例子说明，在银行发放贷款时，会根据申请人的特征决定是否发放**

- 一种极端的情形是申请人**是否有房产**的属性取值和是否同意贷款的分类结果完全吻合，即在训练数据中，每个有房的申请人都对应同意贷款，而每个没房的申请人都对应不同意贷款。这种情况下，“是否有房产”这个特征就具有最大的信息增益，它完全消除了分类结果的不确定性。在处理测试实例时，只要根据这个特征就可以确定分类结果，甚至无需考虑其他特征的取值
- 另一种极端的情形是**申请人的年龄和是否同意贷款的分类结果可能完全无关**，即  
  在训练数据中，青年 / 中年 / 老年每个年龄段内，同意贷款与不同意贷款的样本数目都大致相等。这相当于分类结果在年龄特征每个取值上都是随机分布的，两者之间没有任何规律可言。这种特征的信息增益很小，也不具备分类能力。一般来说，抛弃这样的特征对决策树学习精度的影响不大

---

**决策树生成：主要就是一些决策树生成算法**

- ID3算法
- C4.5算法
- **CART算法（分类与回归树）**

---

**决策树剪枝：决策树剪枝是通过主动去掉分支以降低过拟合的风险，提升模型的泛化性能，分为两种**

- **预剪枝**：在决策树的生成过程中，在划分前就对每个节点进行估计，如果当前节点的划分不能带来泛化性能的提升，就直接将当前节点标记为叶节点；优点时降低过拟合并减少时间开销；缺点时可能矫枉过正，因为某些分支虽然当前看起来没用，在其基础上的后续划分却可能让泛化性能显著提升
- **后剪枝**：是先从训练集生成一棵完整的决策树，计算其在验证集上的分类精  
  度，再在完整决策树的基础上剪枝，通过比较剪枝前和剪枝后的分类精度决定分支是否保留

## （5）支持向量机

### ①：简述

**支持向量机：超平面是分割输入变量空间的线。 在SVM中，会选出一个超平面以将输入变量空间中的点按其类别（0类或1类）进行分离。在二维空间中可以将其视为一条线，所有的输入点都可以被这条线完全分开。SVM学习算法就是要找到能让超平面对类别有最佳分离的系数**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F5e8c846615244378ad33d86342876b47.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

### ②：概述

**支持向量机：支持向量机是一种二分类算法，通过在高维空间中构造超平面实现对样本的分类。在高维的特征空间上，划分超平面可以用简单的线性方程描述**

- **w w w（ n n n维）是法向量**：决定了超平面的方向
- **b b b为截距**：决定了超平面和高维空间中原点的距离  
  ![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fb1cb7d37c57846129088829dd409c28c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**划分超平面将特征空间分为两个部分**

- 位于法向量所指向一侧的数据被划分为正类，其分类标记 y = + 1 y=+1 y\=+1
- 另一侧的数据被划分为负类，其分类标记 y = − 1 y=-1 y\=−1

**支持向量机最简单的情形是训练数据线性可分的情况，此时的支持向量机就被弱化为线性可分支持向量机，线性可分支持向量机就是在给定训练数据集的条件下，根据间隔最大化学习最优的划分超平面的过程**

---

**几何间隔：给定超平面后，特征空间中的样本点 到超平面的距离可以表示为 r = w T X + B ∣ ∣ w ∣ ∣ r=\\frac\{w\^\{T\}X+B\}\{||w||\} r\=∣∣w∣∣wTX+B​，这个距离是个归一化的距离，因而被称为几何间隔**

**函数间隔：通过合理设置参数 w w w和 b b b ，可以使每个样本点到最优划分超平面的距离都不小于 \-1，即满足以下关系。式中的距离是非归一化的距离，被称为函数间隔。**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F3f20e0d8c633437b8097cbbc7360c3cf.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**支持向量：在特征空间中，距离划分超平面最近的样本点能让上式取得等号，这些样本被称为“支持向量”，两个异类支持向量到超平面的距离之和为 2 ∣ ∣ w ∣ ∣ \\frac\{2\}\{||w||\} ∣∣w∣∣2​**

---

**线性支持向量机：线性可分支持向量机是使硬间隔最大化的算法。在实际问题中，训练数据集中通常会出现噪声或异常点，导致其线性不可分，所以去掉可分条件后就变为了线性支持向量机，其的通用性体现在将原始的硬间隔最大化策略转变为软间隔最大化，在线性不可分的训练集中，导致不可分的只是少量异常点，只要把这些异常点去掉，余下的大部分样本点依然满足线性可分的条件**

---

**非线性问题：不论是线性可分支持向量机还是线性支持向量机，都只能处理线性问题，对于非线性问题则无能为力。可如果能将样本从原始空间映射到更高维度的特征空间之上，在新的特征空间上样本就可能是线性可分的。如果样本的属性数有限，那么一定存在一个高维特征空间使样本可分。将原始低维空间上的非线性问题转化为新的高维空间上的线性问题，这就是核技巧的基本思想**

**非线性变换（核技巧）假定原始空间是低纬欧几里得空间，新空间是高纬希尔伯特空间，那么此变换可以用核函数表示为映射函数内积的形式**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F56f70c833c2d4390b49c57d6dd78061e.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**核函数主要有**

![blog.csdnimg.cn/866dafaeaa934e8a87f393893e5aa7f0.png)](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F3e764312dbf14a548000dc0019552cc4.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

## （6）集成学习

**集成学习：集成学习是使用多个个体学习器来获得比每个单独学习器更好的预测性能**

- **类似于无线通信中MIMO传输技术（多输入多输出）**：其早期配置是在发送端和接收端同时布置多个发射机和多个接收机，每个发射机发送相同的信号副本，而每个接收机则接收到来自多个发射机的不同信号，这些信号经历的衰减是相互独立的。这样一来，在接收端多个信号同时被严重衰减的概率就会以指数形式减小，通过获得分集增益带来误码率的下降与信道容量的提升

**集成学习的作用是将多个可以得到的假设整合为单个更好的假设，其一般结构是先产生一组个体学习器，再使用某种策略将它们加以整合**

- **同质集成**：每组中的个体学习器属于同一类型（比如都是线性回归或者都是决策树）
- **异质集成**：由不同类型学习器得到的集成

**集成学习如何实现“1+1 > 2”的效果，这便对个体学习器提出了一些要求**

- **个体学习器性能要有一定保证**
- **个体学习性能要有一定的差异**：这是因为多样性是不同的个体学习器

**集成学习的一个重要前提是个体学习器误差相互独立，但由于个体学习器是为了解决相同问题训练出来的，要让它们的性能完全独立着实是勉为其难。尤其是当个体学习器的准确性较高时，要获得多样性就不得不以牺牲准确性作为代价。由此，集成学习的核心问题在于在多样性和准确性间做出折中，进而产生并结合各具优势的个体学习器**

---

**Boosting和Bagging：集成学习方法可以分为两类：个体学习器间存在强依赖关系因而必须串行生成的序列化方法，和个体学习器之间不存在强依赖关系因而可以同时生成的并行化方法**

- **提升（Boosting）**：序列化方法中的数据使用机制被称为提升（Boosting），其基本思路是对所有训练数据进行多次重复应用，每次应用前需要对样本的概率分布做出调整，以达到不同的训练效果
- **打包（Bagging）** ：并行化方法中的数据使用机制是将原始的训练数据集拆分成若干互不交叠的子集，再根据每个子集独立地训练出不同的个体学习器。这种方法被称为自助聚合（Bootstrap AGgregation），简称打包（Bagging）

**典型的序列化学习算法是自适应提升方法（Adaptive Boosting）；典型的并行化学习方法是随机森林方法**

## （7）聚类

略

## （8）降维

**降维：所谓降维就是指抓住事物的主要矛盾，因为一个学习任务通常会涉及样本的多个属性，但并非每个属性在问题的解决中都具有同等重要的地位，有些属性可能举足轻重，另一些则可能无关紧要。当然，降维有好处也有坏处，以“学位”和“学历”为例**

- 学位和学历代表着两个独立的过程，因此单独列出是没有问题的。但在我国现行的惯例下，这两者通常会一并取得。两者之间的相关性足以让我们根据一个属性的取值去推测另一个属性的取值，因此只要保留其中一个就够了
- 如果毕业论文的答辩没有通过，就会出现只有学历而没有学位的情形；对于在职研究生来说，只有学位没有学历的情形也不稀奇这说明如果将学历和学位完全等同，就会在这些特例上出现错误，也就意味着**信息的损失**

---

**主成分分析：主成分分析是一种主要的降维方法，它利用正交变换将一组可能存在相关性的变量转换成一组线性无关的变量，这些线性无关的变量就是主成分。在实际的数据操作中，主成分分析解决的就是确定以何种标准确定属性的保留还是丢弃，以及度量降维之后的信息损失**

**从几何意义来看，主成分分析是要将原始数据拟合成新的 n n n维椭球体，这个椭球体的每个轴代表着一个主成分。如果椭球体的某个轴线较短，那么该轴线所代表的主成分的方差也很小。在数据集的表示中省略掉该轴线以及其相应的主成分，只会丢失相当小的信息量**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F0e1a7da8382842ce9ebcb4d869cac01b.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**具体说来，主成分分析遵循如下的步骤**

- **数据规范化**：对 m m m个样本的相同属性值求出算数平均值，再用原始数据减去平均数，得到规范化后的数据

- **协方差矩阵计算**：对规范化后的新样本计算不同属性之间的协方差矩阵，如果每个样本有 n n n个属性，得到的协方差矩阵就是 n n n维矩阵

- **特征值分解**：求解协方差矩阵的特征值和特征向量，并将特征向量归一化为单位向量

- **降维处理**：将特征值按降序排序，保留其中最大的 k k k个，再将其对应的 k k k个特征向量分别作为列向量组成特征向量矩阵

- **数据投影**：将减去均值后的 m × n m×n m×n维矩阵和由 k k k个特征向量组成的 n × k n×k n×k维特征向量矩阵相乘，得到的 m × k m×k m×k维矩阵就是原始数据的投影

**经过这几步简单的数学运算后，原始的 n n n维特征就被映射到新的 k k k维特征之上。这些相互正交的新特征就是主成分**

# 四：人工神经网络

- 脑神经科学研究表明，人脑由大约千亿个**神经细胞**及亿亿个**神经突触**组成，这些神经细胞及其突触共同构成了一个**庞大的生物神经网络**。每个神经细胞通过突触与其他神经细胞进行连接与通信。当通过突触所接收到的信号强度超过某个阈值时，神经细胞便会进入激活状态，并通过突触向上层神经细胞发送激活信号。人类所有与意识及智能相关的活动，都是通过特定区域神经细胞间的相互激活与协同工作而现的

**人工神经网络：当下人工智能主流研究方法是连接主义，连接主义学派通过人工构建神经网络的方式来模拟人类智能。它以工程技术手段模拟人脑神经系统的结构和功能，通过大量的非线性并行处理器模拟人脑中众多的神经元，用处理器复杂的连接关系模拟人脑中众多神经元之间的突触行为。大脑思维来源于功能的逐级整合（神经元-神经网络-神经回路-大脑思维），每一层在整合时都会实现“1+1>2”的效果，所以人工神经网络问题的本质就在于如何把大量神经元组装成一个功能系统**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fdb90fa44bccd4e5693c2eb6a2e63d409.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**人类之所以具有智能，是因为有极强的认知能力，这种认知是一种高度抽象化的加工模型。在这个模型中，信息的加工具有从简单到复杂的层次化特征，在每个层次上都有相应的表征，无论是特征提取还是认知加工，都是由不同表征的组合完成的，从信息科学的角度看，整个加工过程可以理解为多次特征提取，提取出的特征从简单到复杂**

**人类智能归根结底还是碳基智能，尽管人工神经网络模拟的是人类神经系统的工作方式，但它归根结底是一套软件，而不是像神经元一样的物质实体，依然要运行在通用的计算机上。所以人类智能和人工智能本质区别在于架构**

- 冯·诺伊曼结构体系的一个核心特征是运算单元和存储单元的分离，所以这意味着**数据的处理和传输无法同时运行**，所以数据总线的传输速度仍然是电脑性能的瓶颈
- 而人大脑数据的传输和处理都由突触和神经元之间的交互完成。重要的是，**数据的传输和处理是同步进行的**

## （1）神经元和感知器

**MP神经元：人工神经网络的基本单位是MP神经元，大脑中的神经元接受电位刺激后会产生沿其轴突传递的神经元动作电位，而MP神经元则是接受一个或多个输入，并对输入的线性加权进行非线性运算以产生输出。假定MP神经元的输入信号是个 N + 1 N+1 N+1维向量 \( x 0 , x 1 , . . . , x N \) \(x\_\{0\},x\_\{1\},...,x\_\{N\}\) \(x0​,x1​,...,xN​\)，第 i i i个分量的权重为 w i w\_\{i\} wi​，则其输出可以写成**

y = ϕ \( ∑ i = 0 N w i x i − θ j \) y=\\phi\(\\sum\\limits\_\{i=0\}\^\{N\}w\_\{i\}x\_\{i\}-\\theta\_\{j\}\) y\=ϕ\(i\=0∑N​wi​xi​−θj​\)

- x 0 x\_\{0\} x0​通常被赋值为+1，也就使 w 0 w\_\{0\} w0​变成固定的偏置输入 b b b
- θ j \\theta\_\{j\} θj​为阈值

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ff88bc926d13942f6a41930f521e7d1cd.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**MP神经元中的函数 ϕ \( \) \\phi\(\) ϕ\(\)被称为激活函数，激活函数通常被设计为连续且有界的非线性增函数。在MP神经元中，输入和输出都限定为二进制信号，使用的传递函数是不连续的符号函数**

- **当输入大于阈值时，符号函数输出 1，反之则输出 0**。这样一来，MP 神经元的工作形式就类似于数字电路中的逻辑门，能够实现类似“逻辑与”或者“逻辑或”的功能，因而又被称为“阈值逻辑单元”

---

**感知器模型：感知器是一种二分类监督学习算法，能够决定由向量表示的输入是否属于某个特定类别。作为第一个用算法精确定义的神经网络，感知器由输入层和输出层组成。输入层负责接收外界信号，输出层是 MP 神经元，也就是阈值逻辑单元。每个输入信号（也就是特征）都以一定的权重被送入 MP 神经元中，MP 神经元则利用符号将特征的线性组合映射为分类输出。给定一个包含若干输入输出对应关系实例的训练集时，感知器引入了学习机制，能够通过权重的调整提升分类的效果**

**感知器有很多优点**

- 当数据**线性可分**时，感知器可以在有限次迭代后收敛，并且得到的决策面是位于两类之间的超平面
- 感知器具有**非参数化特性**，这意味着即使输入数据是非高斯分布时，算法依然能够正常工作
- 感知器具有**自适应性**，只要给定训练数据集，算法就可以基于误差修正自适应地调整参数而无需人工介入

---

**异或操作是一种两输入的逻辑操作**：当两个输入不同时，输出为真；而当两个输入相同时，输出为假

**异或操作可以放在包含四个象限的平面直角坐标系下观察**：

- 在第一象限和第三象限中，横坐标和纵坐标的符号相同；
- 在第二象限和第四象限中，横坐标和纵坐标的符号相反

这样一来，一三象限上的两个点`(1, 1)` 和 `(-1, \-1)` 就可以归为一类，二四象限上的两个点 `(-1, 1)` 和 `(1, \-1)` 则可以归为另一类。划分这四个点就是一个**二分类问题**

**异或问题：这不是一个线性分类问题，因为找不到任何一条直线能将正方形中两组对角线上的顶点分为一类。感知器有以下两个关键问题**

- 单层感知器无法解决以异或为代表的线性不可分问题
- 受硬件水平的限制，当时的计算机无法完成训练感知器所需要的超大的计算量

## （2）多层感知器

**多层感知器：单层感知器无法解决异或问题，但多层感知器可以。多层感知器包含一个或多个在输入和输出节点之间的隐藏层，除了输入节点外，每个结点都是一个使用非线性激活函数的神经元，而在不同层之间，多层感知器具有全连接性，即任意层中的每个神经元都与它前一层中的所有神经元或节点相连，连接的强度由权重系数决定。多层感知器是一类前馈人工神经网络。网路中每一层神经元的输出都指向输出方向，也就是向前馈送到下一层，直到获得整个网络的输出为止。注意多层感知器采用对数几率函数作为激活函数**

- 下图是一个包含三个层次的神经网络。红色的是输入层，绿色的是输出层，紫色的是中间层（也叫隐藏层）

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F57bc776474904a2db75d2718a6d88773.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**如下图，这种包含单个隐藏层的神经网络可以完美解决异或问题**

- 假定两个输入结点A和B的二进制输入分别为1和0，根据图中权重系数可以计算出神经元C的输入为0.5，D的输入为0
- 在由C和D构成的隐藏层中，由于C的输入大于0，因而符号函数使其输出为1；由于D的输入等于0，因而符号函数使其输出为0（图中画错了）
- 在输出节点的神经元上，各路输入线性组合的结果为0.5，因而E的（也即神经网络整体输出）为1
- 这与0异或1的结果完全相符

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F31dc6955b5e64ac2af7773d137c9571b.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**反向传播算法**：多层感知器的训练借助反向传播算法进行，**反向传播算法通过求解误差函数关于每个权重系数的偏导数，以此使误差最小化来训练整个网络**

反向传播算法需要明确误差函数的形式，对于单层感知器误差可以直接被定义为两者之间的差值，但在多层感知器中层与层之间误差有正有负，所以误差可能会部分或完全抵消，因此在反向传播算法中，**每个输出神经元的误差都被写成平方项的形式，整个神经网络的误差则是所有输出神经元的误差之和**

**求解误差函数的最小值就要找到误差函数的梯度，再根据梯度调整权重系  
数，使误差函数最小化**。对误差函数的求解从输出节点开始，通过神经网络逆向传播，直到回溯到输入节点。这对应的是**求导的链式法则**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F8530cbd6875947f9bd646b0eeb5fa024.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**反向传播算法流程**

- 初始化网络中所有权重系数和阈值
- 在前向计算中，将训练样本送入输入节点，在输出节点得到训练结果，再以**平方误差形式**计算训练输出和真实输出之间的误差函数 E = 1 2 ∑ j \( d j − y j \) 2 E=\\frac\{1\}\{2\}\\sum\\limits\_\{j\}\(d\_\{j\}-y\_\{j\}\)\^\{2\} E\=21​j∑​\(dj​−yj​\)2
- 在方向计算中，计算神经网络的局域梯度 ∂ E ∂ w h i \\frac\{\\partial E\}\{\\partial wh\_\{i\}\} ∂whi​∂E​，并根据局域梯度和学习率 η \\eta η从**输出层到隐藏层**对权重系数进行逐层更新 Δ w h i = − η ∂ E ∂ w h i \\Delta w\_\{hi\}=-\\eta \\frac\{\\partial E\}\{\\partial w\_\{hi\}\} Δwhi​\=−η∂whi​∂E​
- 利用新样本训练多层感知器，迭代进行前向计算和反向计算，知道满足停止准则

## （3）径向基函数神经网络

- **多层感知器全局作用**：多层感知器是一类**全局逼近的神经网络**，网络的每个权重对任何一个输出都会产生同等程度的影响。因而对于每次训练，网络都要调整全部权值，这就造成全局逼近网络的**收敛速度较慢**。显然，这是一种牵一发而动全身的全局作用方式
- **局部作用**：在局部作用中，每个局部神经元只对特定区域的输入产生响  
  应。如果输入在空间上是相近的，对这些输入的反应应该是相似的，那么被这些输入激活的神经元也应该是同一批神经元

---

**径向基函数神经网络（RBFN）：在神经科学中，有一个概念叫做感受野，一个感觉神经元的感受野指的是位于这一区域内的适当刺激能够引起该神经元反应的区域。人类神经的感受野的变化方式可以在人工神经网络中以权重系数的形式体现出来，所以按感受野的变化规律设置权重系数，得到的便是径向基函数神经网络**

**径向基网络通常包含三层：一个输入层、一个隐藏层和一个输出层。其中隐藏层是径向基网络的核心结构。每个隐藏神经元都选择径向基函数作为传递函数，对输入分量的组合加以处理。需要注意的是，输入节点和隐藏节点之间是直接相连的，权重系数为 1**

---

**径向基函数：数是只取决于与中心矢量的距离的函数，也就是不管不同的点是在东西还是南北，只要它们和中心点之间的距离相同，其函数值就是相同的。以欧氏距离为度量标准，函数可以被定义为平缓变换的高斯函数**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F1d60ffeab4dd4674ac0e21c031d2f7d5.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**径向基函数可以将低维空间上的线性不可分问题转化为高纬空间上的线性可分问题，这和机器学习中SVM思路是一致的**

---

**径向基函数神经网络训练：主要有两个步骤**

- **训练隐藏层**：使用K-均值聚类这一无监督学习方法，聚类的数目 K 决定了隐藏神经元的数目，通过这个参数设计者可以控制径向基网络的性能和计算复杂度。算法的参数确定后，就能够对训练数据进行无监督的分类，计算出的每个聚类的中心就是高斯函数的中心
- **训练输出层**：用线性模型拟合初始化的隐藏层中的各个中心向量，拟合的损失函数设定为最小均方误差函数，使用递归最小二乘法

**在训练完成后还可以添加额外的一个步骤，利用反向传播算法对径向基网络的所有参数进行一次微调，以达到更好的训练效果**

## （4）自组织特征映射

**自组织特征映射：无论是全局逼近的多层感知器，还是局部逼近的径向基网络，在训练中用到的都是监督学习的方法。如果将无监督学习引入神经网络中，对应的结构就是自组织特征映射。它有两个明显不同**

- 能够将高维的输入数据映射到低维空间之上（通常是二维空间），因而起到**降维**的作用。在降维的同时，自组织映射还能**维持数据在高维空间上的原始拓扑**，将**高维空间中相似的样本点映射到网络输出层的邻近神经元上**，从而保留输入数据的结构化特征

- 自组织映射采用的是**竞争性学习**而非传统的纠错学习。在竞争性学习中，**对输入样本产生响应的权利并不取决于预设的权重系数，而是由各个神经元相互竞争得到的**。不断竞争的过程就是网络中不同神经元的作用不断专门化的过程。**竞争过程的实质是找到输入模式和神经元之间的最佳匹配**

  - 竞争性学习来源于生物的神经系统中的**侧向抑制**效应

**自组织映射的拓扑结构并非如多层感知器般的层次结构，而是一张一维或者二维的网格，网格中的每个节点都代表一个神经元，神经元的权重系数则是和输入数据的维度相同的向量。在拓扑结构中，每个神经元的位置都不是随意选取的，而是和功能有着直接的关系。距离较近的神经元能够处理模式相似的数据，距离较远的神经元处理对象的差异也会很大**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fe9f83d1bcaed4689b12fa6c86ff34af8.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**自组织映射的主要任务就是将任意维度的输入模式转换为一维或二维的离散映射，并以拓扑有序的方式自适应地实现这个映射。当初始化完成后，网络的训练就包括以下三个主要过程**

- **竞争过程**：对每个输入模式，网络中的神经元都计算其判别函数的取值，判别函数值最大的神经元成为竞争过程的优胜者
- **合作过程**：获胜神经元决定兴奋神经元的拓扑邻域的空间位置；
- **自适应过程**：兴奋神经元适当调节其权重系数，以增加它们对于当前输入模式的判别函数值，强化未来对类似输入模式的响应

## （5）模糊神经网络

- **模糊集合**：在不模糊的集合里，每个元素和集合之间的隶属关系是明确的，也就是要么属于集合，要么不属于集合，两者之间泾渭分明。可在模糊集合中，元素和集合之间的关系不是非此即彼的明确定性关系，而是用一个叫做**隶属度**的函数定量表示

**模糊神经网络：是一类特殊的神经网络，它是神经网络和模糊逻辑结合形成的混合智能系统，通过将模糊系统的类人推理方式与神经网络的学习和连接结构相融合来协同这两种技术。简单来说，模糊神经网络就是将常规的神经网络赋予模糊输入信号和模糊权值，其作用在于利用神经网络结构来实现模糊逻辑推理。在网络内部，处理输入信号和权重系数的则是模糊数学，隐藏神经元表示的就是隶属函数和模糊规则**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F6db4ef8ebd2145ddba16a54d18f59c2b.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

# 五：深度学习

**深蓝之所以能够战胜国际象棋高手，是因为它具有强大的运算能力，利用强大的计算资源来优化目标函数。深蓝本身就是一套专用于国际象棋的硬件，大部分逻辑规则是以特定的象棋芯片电路实现，辅以较少量负责调度与实现，高阶功能的软件代码。其算法的核心是暴力穷举：生成所有可能的下法，然后执行尽可能深的搜索，并不断对局面进行评估，尝试找出最佳的一手**

**围棋比国际象棋规则复杂得多，所以以穷举法进行最优落子策略的推演无异于痴人说梦，而围棋高手在下棋时除了逻辑推理外，更多地还是依靠直接来落子，而对计算机来说依靠直觉那是不可能的事情，知道AlphaGo横空出世**

**AlphaGo的核心在于“想”，与专用硬件深蓝不同，AlphaGo 是一套能够运行在通用硬件之上的纯软件程序。它汲取了人类棋手海量的棋谱数据，并依赖人工神经网络和深度学习技术从这些数据中学会了预测人类棋手在任意的棋盘状态下走子的概率，模拟了以人类棋手的思维方式对棋局进行思考的过程。而与深蓝有所区别，AlphaGo对围棋规则一无所知，在整个算法中，只有获胜这个概念，所以AlphaGo 是几乎没有特定领域知识的、基于机器学习的、更加通用的人工智能**

**深度学习：AlphaGo 的胜利也是深度学习的胜利。深度学习是利用包含多个隐藏层的人工神经网络实现的学习。正是这“多个”隐藏层给深度学习带来了无与伦比的优势。与人工神经网络一样，深度学习思想同样来源于生理学上的研究进展，发现了方向选择性细胞，这意味着可视皮层是分级的**

- **方向选择性细胞表明**：神经 - 中枢 - 大脑的工作过程，或许正是一个不断迭代、不断抽象的过程。人眼处理来自外界的视觉信息时，首先提取出目标物的边缘特性，再从边缘特性中提取出目标物的特征，最后将不同特征组合成相应的整体，进而准确地区分不同的物体。在这个过程中，高层特征是低层特征的组合，从低层到高层，特征变得越来越抽象，语义的表现就越来越清晰，对目标物的识别也就越来越精确

**在深度学习中，这个过程可以利用多个隐藏层进行模拟**

- 第一个隐藏层学习到“**边缘**”的特征
- 第二个隐藏层学习到的是由“边缘”组成的“**形状**”的特征
- 第三个隐藏层学习到的是由“形状”组成的“**图案**”的特征
- 最后的隐藏层学习到的是由“图案”组成的“**目标**”的特征

**深度学习在近近十年发展如此迅速，主要得益于数据的井喷和计算能力的飙升，所以深度学习+大数据可谓是天作之合。而基于向量和矩阵计算的图形处理单元（graphical processing unit, GPU）完美解决了这一问题，其庞大的处理吞吐量能够使深度神经网络的训练显著提速，因而也成了深度学习的标配硬件**

**所以，如果说深度学习是一台探矿机，大数据就是那座有待挖掘的金矿，计算能力的进展则为这台探矿机提供了源源不断的动力**

## （1）深度前馈网络

**深度前馈网络（Deep Feedforward Network）：是具有深度结构的前馈神经网络，可以看成是进化版的多层感知器。与只有一个或两个隐藏层的浅层网络相比，深度前馈网络具有更多的隐藏层数目，从而具备了更强的特征提取能力。深度前馈网络不考虑输入数据可能具备的任何特定结构，也就是不使用关于数据的先验信息。但特征提取能力增强的代价是运算复杂度的提升。因而，网络架构的建立、损失函数的选择、输出单元和隐藏单元的设计、训练误差的处理等问题就成为深度前馈网络设计中的一系列核心问题。深度前馈网络的出现克服的正是单隐藏层带来的复杂性问题：使用深度架构的模型既能减少表示目标函数时所需要的单元数量，也能有效降低泛化误差，在一定程度上抑制过拟合的发生**

**任何机器学习算法都可以看成是对某个预设函数的最优化方法，深度前馈网络也不例外。与其他神经网络一样，深度前馈网络也利用梯度信息进行学习，在处理误差时采用的是反向传播方法，利用反向传播求出梯度后再使用随机梯度下降法寻找损失函数的最小值**

**深度前馈网络选择损失函数的准则与其他机器学习算法并没有什么区别：回归问题的损失函数通常是最小均方误差，而分类问题的损失函数通常是交叉熵，其实无论是最小均方误差还是交叉熵，体现的都是概率论中的最大似然估计原理**

**在深层前馈网络的设计中，一个独有的问题就是隐藏单元的设计，也就是隐藏神经元的激活函数如何选择。整流线性单元是隐藏单元理想设计的万金油，当你不知道选择何种激活函数时，那么用它准保没错**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fdece9e2dcb0142c089f20009b41c1c27.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

## （2）深度学习中的正则化

**正则化（Regularization）：作为抑制过拟合的手段，是机器学习和深度学习之中必不可少的环节。正则化就是一类通过显式设计降低泛化误差，以提升算法通用性的策略的统称。从概率论角度看，许多正则化技术对应的是在模型参数上施加一定的先验分布，其作用是改变泛化误差的结构**

**正则化策略可以分为以下几类**

- **基于训练数据的正则化**：对训练数据正则化的做法是在训练数据集上施加变换，从而产生新的训练数据集，最简单的实例就是向数据添加随机的高斯噪声

- **基于网络架构的正则化**：通常会简化关于映射的假设，再让网络架构逐步逼近简化后的映射。对映射的假设既可以关注深度网络中不同层次的具体操作，也可以关注层与层之间的连接方式

- **基于参数共享的正则化**：是一类重用参数的正则化方法。通过强迫某些参数相等，可以让不同模型共享唯一的参数，从而让它们对相似的输入产生相似的输出。如果放宽参数共享的条件，使它们不必完全相等而是相互接近，对应的就是对参数范数添加正则化项

- **基于激活函数的正则化**：一些传递函数是专门为正则化设计的，比如在 Dropout 中使用的 maxout 单元，它能在测试时更精确地近似模型集合预测结果的几何平均值

- **基于误差函数和基于正则化项的正则化**：理想情况下，误差函数  
  应当适当地反映算法的性能，并体现出数据分布的一些特点（比如均方误差或交叉熵）。对误差函数进行正则化就相当于添加额外的学习任务，从而导致其目标发生变化，这部分变化就会体现在误差函数中额外的正则化项上。因而在大部分情况下，对基于正则化项的正则化的讨论就包含了基于误差函数的正则化

  - 正则化项也叫做**惩罚项**，与误差函数不同，正则化项与目标无关，而是用于表示所需模型的其他属性。常用的正则化项是**权重衰减项**，深度学习中的参数包括每个神经元中的**权重系数和偏置**。由于每个权重会指定两个变量之间相互作用的方式，因而拟合权重所需要的数据量要比拟合偏置多得多。在权重衰减中，正则化项是以**范数**的形式表示的，常用的范数包括 l 2 l\_\{2\} l2​范数和 l 1 l\_\{1\} l1​范数

- **基于最优化过程的正则化**：根据其作用阶段的不同，这类正则化方法可以分为三种

  - **对初始化正则项**
  - **对参数更新的正则化**
  - **对终止条件的正则化**

## （3）深度学习中的优化

**优化（Optimization）**：由于深度神经网络中的**隐藏层数目较多**，因而将整个网络作为一个整体进行优化是非常困难的事情，需要花费大量的时间和计算力。出于效率和精确性的考虑，**在深度学习的优化上需要使用专门的技术**。传统机器学习算法往往会小心翼翼地选择代价函数和优化条件，将待优化问题转化为容易求解的**凸优化**问题。但在神经网络，尤其是在深度神经网络中，更一般的非凸情况是不可避免的，这就给深度学习中的优化带来很多额外的挑战。**在深度学习优化中以下几类问题是比较头疼的**

- **病态矩阵**：在线性方程 A x = b Ax=b Ax\=b中，**当系数矩阵 A A A的微小扰动会给解集 x x x带来较大浮动的波动时**，这样的矩阵就称为病态矩阵。在神经网络的训练中，病态矩阵的影响体现在**梯度下降的不稳定性上**。当应用随机梯度下降解决优化问题时，**病态矩阵对输入的敏感性会导致很小的更新步长也会增加代价函数，使学习的速度变得异常缓慢**

- **局部极小值**：凸优化问题的数学特性保证了局部极小值和全局最小值之间的等价关系。但在神经网络，尤其是深度模型中，**代价函数甚至会具有不可列无限多个局部极小值，这显然会妨碍对全局最小值的寻找，导致搜索陷入局部最优的陷阱中**。神经网络之所以会具有这么多局部极小值，原因在于隐藏变量的不可辨认性。如果将神经网络中的几个隐藏神经元及其所有系数调换的话，得到的新模型和原始模型之间是等价的。**假定深度模型中包含 m m m个隐藏层，每个层中又都有 n n n个神经元，那么隐藏单元的排列方式会多达 \( n \! \) m \(n\!\)\^\{m\} \(n\!\)m种。这么多神经网络的变体是没法相互区分的，因而它们都有相同的局部极小值**

- **鞍点**：鞍点是梯度为 0 的临界点，**但它既不是极大值也不是极小值**。由于牛顿法的目标是寻找梯度为零的临界点，因而会受鞍点的影响较大，**高维空间中鞍点数目的激增就会严重限制牛顿法的性能**

---

**随机梯度下降法**：是在传统机器学习和深度神经网络中都能发挥作用的经典算法。**如果把求解最优化问题想象成爬山，那么随机梯度下降法就是每走一步就换个方向**。为了节省每次迭代的计算成本，随机梯度下降在每一次迭代中都使用**训练数据集的一个较小子集来求解梯度的均值，这在大规模机器学习问题中，特别是深度学习中非常有效**。虽然不是每次迭代得到的结果都指向全局最优方向，但大方向终归是没有错的，其最终的结果往往也在全局最优解附近。**在随机梯度下降法的基础上进行改进可以得到其他的优化方式，改进的手段主要有以下两种**

- **随机降低噪声**
- **使用二阶导近似**

## （4）自编码器

**自编码器\(auto-enconder\)： 在深度学习中，自动编码器是一种无监督的神经网络模型，它可以学习到输入数据的隐含特征，这称为编码\(coding\)，同时用学习到的新特征可以重构出原始输入数据，称之为解码\(decoding\)。在结构上，自编码器是包含若干隐藏层的深度前馈神经网络，其独特之处是输入层和输出层的单元数目相等；在功能上，自编码器的目的不是根据输入来预测输出，而是重建网络的输入**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fd37d7cdd9bfe4312b39979f374f730c6.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**并根据隐藏层和输入数据维度不同，可降自编码器分为**

- **欠完备自编码器**：隐藏层的维度小于输入数据的维度，相当于对输入信号做了主成分分析
- **过度完备自编码器**：自编码器的隐藏单元数目大于输入信号的维度，也就是编码映射的分量数目大于输入信号的分量数目

**自编码器结构由编码映射和解码映射两部分组成。如果将编码映射记作 ϕ \\phi ϕ，解码映射记作 ψ ψ ψ，自编码器的作用就是将输入 X X X改写为 \( ψ ○ ϕ \) \( X \) \(ψ ○ \\phi\)\(X\) \(ψ○ϕ\)\(X\)。如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F6a3d5bed70f645858c0f469ccad2124f.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**深度自编码器：自编码器通常只包含单个隐藏层，但这并不是强制的选择，自编码器也会受益于深度结构，所以深度自编码器既能降低某些函数的计算成本，也能降低训练数据的数量要求，还能得到更高的压缩效率。在实际中，训练深度自编码器的普遍策略是先训练一些浅层自编码器，再利用这些浅层自编码器贪心地预训练深度结构，因而浅层自编码器可以看作深度自编码器的中间件。用浅层自编码器搭建成的深度自编码器被称为栈式自编码器，栈式自编码器的训练策略可以归结为两个步骤**

- 无监督预训练
- 有监督微调

## （5）深度信念网络

**深度信念网络：深度信念网络是一种概率生成模型，能够建立输入数据和输出类别的联合概率分布。网络中包含多个隐藏层，隐藏层中的隐藏变量通常是二进制数，用来对输入信号进行特征提取。输入信号从深度信念网络的最底层输入，并自下而上有向地传递给隐藏层。而在网络最上面的两层中，神经元之间的连接是没有方向并且对称的，这两个层次共同构成了联想记忆**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F18d7c1cdf8d74b3ea3b0a441afa0ff71.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**受限玻尔兹曼机**：从结构上看，复杂的深度信念网络可以看成由若干简单的学习单元构成的整体，而构成它的基本单元就是**受限玻尔兹曼机**。受限玻尔兹曼机的模型非常简单，就是一个**两层的神经网络**，包括**一个可见层和一个隐藏层**（可见层用来接收数据，隐藏层则用来处理数据）

- **受限**：任意两个不同层次中的神经元都会两两相连，但同一层中的神经元则不会互相连接，因而每个层内也就没有信息流动

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F0fcb8cc06eb947318d92206e2ca69165.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**对比散度方法**：除了得到隐藏神经元的输出外，受限玻尔兹曼机还要**以无监督的方式对数据进行重构**，即使没有更深层的网络结构，**数据也会在输入层和隐藏层中进行多次前向和反向的传递**。具体来说，在隐藏神经元得到输出后，受限玻尔兹曼机需要将输出结果**反馈给可见层**，也即**保持所有连接的权重系数不变，但是将方向反转**，这样一来，每个隐藏单元的输出就会按照已经确定的系数反馈给可见层，可见层的每个神经元接收到的反馈信息是不同隐藏单元输出的线性组合。反馈信息和一组新的偏置分量求和就得到了对原始输入的估计，估计值和原始输入的差值则表示了重构误差。**通过让重构误差在可见层和隐藏层之间循环往复地传播，就可以求出使重构误差最小化的一组权重系数**。这便是**对比散度方法**，它**既能让隐藏层准确地提取可见层的特征，也能根据隐藏层的特征较好地还原出可见层**

**所以，将几个受限玻尔兹曼机堆叠在一起，就可以得到深度信念网络**。除了最顶层和最底层外，深度信念网络的每个隐藏层都扮演着双重角色：**它既作为之前神经元的隐藏层，也作为之后神经元的可见层**

## （6）卷积神经网络

**卷积**：卷积是对**两个函数**进行的一种数学运算，在不同的学科中有不同的解释方式，**在卷积网络中，两个参与运算的函数分别叫做输入和核函数**。本质上讲，卷积就是以**核函数作为权重系数，对输入进行加权求和的过程。可以把卷积看作是是做菜，输入函数是原料，核函数则是菜谱，不同的菜谱对应不同的口味，不同的核函数也对应不同的输出**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F627c0a0c311a43bf8dabaa2e9da76d78.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**图像上的卷积**：卷积运算之所以应用于图像识别当中，是因为它具有一些优良的性质

- **系数感知性**：卷积层核函数的大小通常远远小于图像的大小
- **参数共享性**：是在一个模型中使用相同的参数，也即在每一轮训练中用单个的核函数去和图像的所有分块来做卷积
- **平移不变性**：当卷积的输入产生平移时，其输出等于  
  原始输出做出相同数量的平移，这说明平移操作和核函数的作用是可以交换的

卷积的结果反映的是**输入像素和核函数之间的近似程度**。**卷积的输出越大表明两者之间的相似性越高，输出越小就意味着两者没什么共性**

---

**卷积神经网络（CNN）：指的是至少在某一层中用卷积运算（convolution）来代替矩阵乘法的神经网络。卷积神经网络的结构并非卷积运算的简单组合，而是包含几个功能不同的层次。当输入图像被送入卷积神经网络后，先后要循环通过卷积层\(CONV\)、激活层\(RELU\)和池化层\(POOL\)，最后从全连接层\(FC\)输出分类结果。具体来说，输入层将待处理的图像转化为一个或者多个像素矩阵，卷积层利用一个或多个卷积核从像素矩阵中提取特征，得到的特征映射经过非线性函数处理后被送入池化层，由池化层执行降维操作。卷积层和池化层的交替使用可以使卷积神经网络提取出不同层次上的图像特征。最后得到的特征作为全连接层的输入，由全连接层的分类器输出分类结果**

- 下图CNN的作用是识别出图像里面是个什么东西

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F66aa8d4892c04fc0aa4a0530f9d32fa8.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

---

**卷积层**：是卷积神经网络的**核心部分**，其参数是一个或者多个**随机初始化的核函数**。核函数会逐行逐列地扫描输入图像，**对像素矩阵进行从左到右，从上到下的滑动覆盖**。每一个被核函数的光圈覆盖的区域都是和核函数维度相同的像素组合，并且作为输入和核函数进行卷积。**当核函数将输入图像全部扫描完毕后，计算出的所有卷积结果又可以构成一个矩阵，这个新矩阵就是特征映射**。卷积层得到的特征映射一般会送到**激活层**处理，给系统添加非线性元素。激活层首选的传递函数是**整流线性单元**，它可以激活特征映射中的负值。**通过合理设置核函数的性质，卷积层就能够提取出图像的特征**

![请添加图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F478503ab9c7143869fbf2df6fdb03828.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**池化层**：卷积神经网络的卷积层之间通常周期性地会插入池化层，其作用是对**得到的特征映射矩阵进行筛选**。卷积层给出了核函数和原始图像每个局部之间的近似关系，但这里面真正对图像分析有帮助的只是取值较大，也就是和核函数相似程度较高的部分。因而常见的最大池化的做法**就是将特征映射划分为若干个矩形区域，挑选每个区域中的最大值**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F27326a87408549828f4ab7b43424f1d7.gif&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**卷积层和池化层的循环使用能够实现对图像特征的逐层提取，而根据提取出的特征得到图像的分类与标记则要交给全连接层完成**

## （7）循环神经网络

**循环神经网络（RNN）：其独特之处在于引入了“时间”的维度，因而适用于处理时间序列类型的数据。在CNN中，如果把参数共享调整到时间的维度上，让神经网络使用相同的权重系数来处理具有先后顺序的数据，得到的就是RNN**

**对于一个以时间为自变量的变长数据来说，很难说清楚数据的终点在哪里，或者说终点根本就不存在。这种情况之下，如果对每一个时间点上的数据都计算一次神经网络的权重系数，无疑会带来极大的计算负荷。循环神经网络就是将长度不定的输入分割为等长度的小块，再使用相同的权重系数进行处理，从而实现对变长输入的计算与处理**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F63bb940101fb4443a71c5e9f021ea73c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)

**时间维度上的参数共享可以充分利用数据之间的时域关联性，例如英语中的完形填空就可以利用上下文推出答案。循环神经网络对时域的关联性的利用体现在时刻 t t t的输出既取决于当前时刻的输入，也取决于网络在前一时刻 t − 1 t-1 t−1甚至于更早的输出。从这个意义上讲，循环神经网络引入了反馈机制，因而具有记忆的功能。正是记忆功能使循环神经网络能够提取来自序列自身的信息，这是传统的前馈神经网络所无法做到的**

![](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdn.net%2F20180917195028940%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlYXJ0aG91Z2Fu%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F127945347)