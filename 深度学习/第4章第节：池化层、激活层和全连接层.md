 

- [相关pdf下载（密码7281）](https://url18.ctfile.com/f/22722418-803125355-edf378)

### 文章目录

- [一：池化层](#_4)
- - [（1）上采样和下采样](#1_9)
  - [（2）池化层作用](#2_16)
  - [（3）torch.nn.Maxpool2d](#3torchnnMaxpool2d_37)
- [二：激活层](#_61)
- [三：全连接层](#_70)

# 一：池化层

**池化层：池化层一般会跟在卷积层进行，主要目的是为了减少特征尺寸，也即池化层不是说一定要加，只是说当你有减少特征尺寸的需求时，可以加入池化层**

## （1）上采样和下采样

**在图像处理中，上采样和下采样是两个非常重要的概念**

- **上采样\(upsampling\)**：其主要目的是对图像进行差值，**放大**原图像，使图像变得更加细腻
- **下采样（subsampled）**：其主要目的是**减小**尺寸、减少计算量

## （2）池化层作用

池化层又叫做下采样层，通常会跟在卷积层之后，引入它的目的主要是为了**简化卷积层的输出**。可以这样理解，池化层在卷积层上架起了一个窗口，但是这个窗口相比卷积层来说简单许多，不需要如 w w w、 b b b这样的参数，它只是对窗口范围内的神经元做一些简单操作，例如求和、求最大值等等

因此**池化层可以对感受野内的特征进行筛选**，提取区域内**代表性的特征**，同时又能够有效减小输出特征尺度，进而**减少模型所需要的参数量**

特别注意，池化操作只会改变H 和 W，并会改变C

**池化按操作类型可以分为如下几种**

- **最大池化（max pooling）**：在CV中使用的最多，提取感受野内最大特征值
- **平均池化（average pooling）**：提取感受野内平均特征值
- **求和池化（sum pooling）**：提取感受野内总和特征值
- **自适应池化\(adaptive pooling\)**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F5fcc61c8078d4e3eba7dfaa98d4bbc02.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc74890cbc157485f9931737c62de54d0.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

## （3）torch.nn.Maxpool2d

```python
torch.nn.Maxpool2d(
    kernel_size,  # 最大池化窗口
    stride, # 步长
    padding,  # 填充
    dilation,  # 控制窗口中元素步幅
    return_indics,  # 布尔类型，返回最大值位置索引
    ceil_mode, # 默认为False，表示向下取整
)
```

例如

```python
Pooling_layer = torch.nn.MaxPool2d(kernel_size=2, stride=2)
out = Pooling_layer(out)
print("bach_size：{}, channels：{}, height：{}, width：{}".format(out.shape[0], out.shape[1], out.shape[2], out.shape[3]))
```

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F3318ef3153fd4ff4b859b9ca360d18c3.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

# 二：激活层

**激活层：** 和之前所说过的线性神经网络一样，卷积本质也是一种**线性运算**，如果只用卷积层，那么无非就是 n n n多个矩阵相乘罢了，将无法解决非线性问题，所以我们会加入激活函数\*\*，以提高模型的非线性表达能力\*\*

一般来说，**每个卷积层后面都会跟上一个激活层**，有关激活函数这里就不再细谈了，前面已有详细介绍最长用的激活函数便是ReLU（整流线性单元）

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc67c46077d3d4d28a27c01e5faf5e3a0.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

# 三：全连接层

在卷积神经网络的最后往往会有**一个或多个全连接层**，目的就是为了**根据提取出的特征进行最后的分类**。例如对于一个二分类问题，最后的输出结果会是一个2×1的向量，向量中的每个值对应各个类别的得分，然后将 此输出值送给分类器（例如softmax分类器）

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ffd091ad8283f4711aa6712aaedf15d5f.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

假设得到了三张特征图，那么在进入全连接层时需要把这4×4×3的矩阵展开成一个48×1的向量

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F2155de703ace456d91f9ac2d80d4962c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

此时这48×1向量中的每一个元素就是输入层的一个神经元，然后根据我们计算得到的权重矩阵，对其进行加权求和，就得到了每个分类的得分

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F1c85047ad5cd4e44be37b95067a6a24c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128895784)

然后剩余部分和我们之前所学的就一样了，根据softmax函数进行概率计算