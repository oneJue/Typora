 

### 文章目录

- [一：经典网络结构](#_16)
- - [（1） LeNet-5（CNN开山始祖）](#1_LeNet5CNN_18)
  - [（2）AlexNet](#2AlexNet_55)
  - - [A：简介](#A_57)
    - [B：网络结构](#B_83)
  - [（3）VGGNet](#3VGGNet_146)
  - - [A：简介](#A_148)
    - [B：网路结构](#B_182)
- [二：复杂网络结构](#_279)
- - [（1）ResNet（残差网络）](#1ResNet_281)
  - - [A：简介](#A_283)
    - [B：网络结构](#B_294)
  - [（2）DenseNet](#2DenseNet_393)
  - [（3）InceptionNet v1-v4](#3InceptionNet_v1v4_468)
- [三：轻量型网络结构](#_474)
- - [（1）MobileNet](#1MobileNet_476)
  - [（2）SquuezeNet](#2SquuezeNet_494)
  - [（3）ShuffleNet](#3ShuffleNet_498)

过去几年计算机视觉研究中的大量研究都集中在如何把这些基本构件组合起来，形成有效的卷积神经网络。最直观的方式之一就是去看一些案例，就像很多人通过看别人的代码来学习编程一样，通过研究别人构建有效组件的案例是个不错的办法。实际上在计算机视觉任务中表现良好的神经网络框架往往也适用于其它任务，也许你的任务也不例外。也就是说，**如果有人已经训练或者计算出擅长识别猫、狗、人的神经网络或者神经网络框架，而你的计算机视觉识别任务是构建一个自动驾驶汽车，你完全可以借鉴别人的神经网络框架来解决自己的问题**

1998年**卷积神经网络开山之作LeNet**被提出，但当时受限于算力和数据集，所以它提出之后一直被传统目标识别算法（特征提取+分类器）所压制，终于在沉寂了14年之后的2012年，**AlexNet横空出世**，在ImageNet挑战赛一举夺魁，使得CNN再次被人重视，并且一发不可收拾，不断研究发展

**如下图是卷积神经网络发展概述，主要是如下三个发展趋势**

- 经典网络结构：以LeNet、AlexNet、VGGNet为代表，主要是卷积层的简单堆叠
- 复杂神经网络：以ResNet、InceptionNetV1-V4、DenseNet为代表
- 轻量型神经网络：以MobileNetV1-V3、ShuffleNet、SqueezeNet为代表

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F63563048a59c4bf8ae979165fe9abcec.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

# 一：经典网络结构

## （1） LeNet-5（CNN开山始祖）

**LeNet-5：是一个较简单的卷积神经网络。下图显示了其结构：输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层**

- **注意**：使用的激活函数为tanh

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Faef89129ae534a28b65296e2728d0b9c.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fb376176f7eca4f4fb080b69052de3067.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

```python
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)
        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=84)
        self.fc3 = nn.Linear(in_features=84, out_features=10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

```

## （2）AlexNet

### A：简介

LeNet提出之后，卷积神经网络在CV和ML中就小有名气了，但是那是它并没有开始主导这些领域，这是因为虽然LeNet在小数据集上取到了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究，事实上在上世纪90年代和2012年大部分时间里，神经网络往往被其他机器学习方法超越，例如非常出名的SVM

但在CV中，神经网络和传统机器学习算法仍然是有很大区别的，例如传统机器学习方法并不会把原始像素直接作为输入，而是会有一个非常重要的步骤——**特征工程** 虽然上世纪90年代已经有了一些神经网络的加速卡，但仅仅靠他们还不足以开发出有大量参数的深层多通道多层卷积神经网络。因此，与卷积神经网络这种**端到端**（像素输入分类输出）的系统不同，传统机器学习流水线应该是这样

- 收集数据
- 根据光学、几何学等其他知识手工对数据集进行预处理
- 通过标准特征提取算法，例如SIFT（尺度不变特征变换）、SURF（加速鲁邦特征）输入数据
- 将提取好的特征送入到你喜欢的分类器中，例如SVM

在2012年AlexNet出现之前，图像特征都是机械地算出来的，而有一些研究人员认为**特征本身是可以被学习的**，并且，在合理地复杂性前提下，特征应该有多个共同学习的神经网络层组成，每个层都应该有可以学习的层数。AlexNet便是这样的网络，2012年在ImageNet上夺得第一名，准确率比传统机器学习算法高出不少

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc1a2345f84d642eeb818dfd138f25960.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

和人眼提出物体特征一样，AlexNet的**底层**会提取到如边缘、颜色等基础信息，然后**高层**会建立在这些底层特征上再次提取，以标识更大的特征，例如眼睛、鼻子、草叶等等，而更高层可以检测整个物体，例如人、飞机等等

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F11375d54662f47a6b3e60985bb766a6b.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

**Alex的成功主要归功于以下两点**

- **数据**：2009年ImageNet数据集发布，并发起了ImageNet挑战赛，要求比赛人员需要从100万个样本中训练模型，以区分1000个不同类别的对象，这种规模是前所未有的
- **硬件**：1999年NVIDA发明了GPU，用于为游戏玩家服务，用来加速图形处理。GPU可以优化高吞吐量的4×4矩阵和向量加法，从而服务于基本图形任务，仔细思考，这些数学运算和卷积运算非常相似。因此GPU的出现大大改变了神经网络训练的窘境

### B：网络结构

**AlexNet：Alex网络结构如下图所示，其设计理念和LeNet非常相似，主要区别如下**

- 网络结构要比LeNet5**深很多**
- 由5个卷积层、两个全连接层隐藏层和一个全连接输出层组成
- AlexNet使用ReLU作为激活函数
- AlexNet第一层的卷积核比较大，为11×11，这是因为ImageNet中大多图像要比MNIST图像多10倍以上
- AlexNet使用**dropout算法**来控制全连接层复杂程度，而LeNet5只使用了权重衰减
- AlexNet对图像数据进行了**增广**（例如对一张图像翻转、裁切和变色相等于有了3张以上的图像），这增大了样本量，减少了过拟合

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fec061de081154499a6af67d2d8c2ffd0.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

**细节过程**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F03f425754d06439c887f96fb2f493d48.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

```

## （3）VGGNet

### A：简介

虽然AlexNet证明了深层神经网络网络是有效的，但是它没有提供一个**通用的模板**来指导后续的研究人员设计新的网络。与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。**研究人员开始从单个神经元角度思考问题，发展到整个层，现在又转向块，重复层的模式**

在VGG网络中，通过使用循环和子程序，可以很容易地在任何现代深度学习框架中实现这些重复的架构

**经典CNN的基本组成部分一般是下面的序列**

- 带填充以保持分辨率的卷积层
- 激活函数
- 汇聚层

一个VGG块与之类似，由一系列卷积层组成，后面再加上利用空间下采样的最大汇聚层  
![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F01c2b7254f914d19b8b322e1577c4a6f.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

下面代码中定义了一个名字叫做`vgg_block`的函数来实现一个VGG块

```python
```python
import torch
from torch import nn

# num_convs表示该VGG块内卷积层的数量
def vgg_block(num_convs, in_chaneels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        layers.apeend(nn.ReLu())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)
```

### B：网路结构

**VGGNet：VGG网络结构如下图所示，和AlexNet、LeNet-5一样，VGG网络可以分为两个部分**

- 第一部分主要由卷积层和汇聚层组成
- 第二部分由全连接层组成

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fbf93485c77244711a1764044f9a26230.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

**其优点如下**

- VGG模型**以较深的网络结构，较小的卷积核和池采样域**，使其能够在获得更多图像特征的同时控制参数的个数，避免计算量过多和过于复杂的结构
- 采用小的卷积核进行堆叠，两个3×3卷积核的堆叠相当于5×5卷积核的视野，三个3×3卷积核的堆叠相当于7×7卷积核的视野。这样做可以有更少的参数（3个堆叠的3×3卷积核参数为27个，而一个7×7卷积核参数有49个），另外还拥有了**更多的非线性变换，增强了CNN的学习能力**
- 在卷积结构中，引入1×1卷积核，在不影响输入输出维度的情况下，增强了网络的表达能力，降低了计算力量
- 训练时，先训练级别简单（层数较浅）的A级网络，然后使用A级网络的权重来初始化后面的复杂模型，加快训练收敛速度
- 采用Multi-Scale方法来训练和预测，可以增加训练的数据量，防止模型过拟合，提升预测准确率
  - Multi-Scale（多尺度图像预测）：将图片进行不同尺度的缩放，得到图像金字塔，然后对每层图片提取不同尺度的特征，得到特征图。最后对每个尺度的特征都进行单独的预测
- **VGG可以单独作为特征抽取器或预训练模型来使用**

**VGG有很多的变体**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fa23d78ece36a4a8aa30f1f527e0ff6a8.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

如下，实现一个VGG16，16表示它有13个卷积层和3个全连接层

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F1bd55cae530e47d2bebf38859aa1edc5.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()

        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
		)
	def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

model = VGG16()

```

# 二：复杂网络结构

## （1）ResNet（残差网络）

### A：简介

从LeNet-5、AlexNet、VGG中可以看出，深层网络一般会比浅层网络的效果要好。那么要想进一步提升模型的准确率，那么把网络设计的越深越好不就行了吗。但事实并非如此，如下图，在ImageNet上做过一个实验，对于一个常规的网络，其在56层时的错误率却远远高于20层时的错误率

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fe3ee512944c042c5af92b6c6afcd9a24.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

因此，随着网络层级的不断加深，模型的精度在前期也会随之提升，但是当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这**说明网络变得很深以后，深度网络变得更加难以训练了**。回想神经网络反向传播原理，**神经网络在反向传播中要不断传播梯度，而当网络层数加深时，梯度在传播过程中会逐渐消失，最终导致无法对前面网络层进行有效调整**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fa6e8075f5f44423ba8184d5220917efe.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

### B：网络结构

**ResNet：残差网络的基本结构如下图所示，很明显该图是带有跳跃结构的**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2F60a20df0bb134b4fb28d0c0cb1ee0158.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

假定某段神经网络的输入是 x x x，期望的复杂输出是 H \( x \) H\(x\) H\(x\)，如果要学习这样的模型，则训练的难度会比较大。残差网络的思想是，**如果已经学习到了较为饱和的准确率，或者说当发现下层的误差会变大时，那么接下来的学习目标就转变为恒等映射的学习，也就是使输入 x x x近似于输出 H \( x \) H\(x\) H\(x\)，以保证在后面的层次中不会造成精度的下降**。在上图中，通过“shortcut connections”的方式，直接把输入 x x x传到输出作为初始结果，输出结构为 H \( x \) = F \( x \) + x H\(x\)=F\(x\)+x H\(x\)\=F\(x\)+x，当 F \( x \) = 0 F\(x\)=0 F\(x\)\=0时，那么 H \( x \) = x H\(x\)=x H\(x\)\=x

因此，残差网络将学习目标改变了：**不再是学习一个完整的输出，而是目标值 H \( x \) H\(x\) H\(x\)和 x x x的差值，也即残差:= H \( x \) = x H\(x\)=x H\(x\)\=x，因此后续的训练目标就是要将残差结果逼近于0，使得随着网络加深，准确率不下降**

这种残差跳跃式的结构，打破了传统的神经网络n-1层的输出只能给n层作为输入的惯例，**使某一层的输出可以直接跨过几层作为后面某一层的输入，其意义在于为叠加多层网络而使得整个学习模型的错误率不降反升的难题提供了新的方向**

**下图是一个34层的深度残差网络结构图，下图中的实线和虚线分别表示**

- 实线：表示通道相同，所以 H \( x \) = F \( x \) + x H\(x\)=F\(x\)+x H\(x\)\=F\(x\)+x
- 虚线：表示通道不同，所以 H \( x \) = F \( x \) + W x H\(x\)=F\(x\)+Wx H\(x\)\=F\(x\)+Wx，W为卷积操作

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fb00c76a8e5464c00985d41490fd6fd9e.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 基本跳连模块
class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3
		out = self.layer4(out) out = F.avg_pool2d(out, 4) 
		out = out.view(out.size(0), -1) 
		out = self.linear(out) return out

def ResNet18(): 
	return ResNet(BasicBlock, [2,2,2,2])
	
def ResNet34(): 
	return ResNet(BasicBlock, [3,4,6,3])
	
def ResNet50(): 
	return ResNet(Bottleneck, [3,4,6,3])

def ResNet101(): 
	return ResNet(Bottleneck, [3,4,23,3])

def ResNet152(): 
	return ResNet(Bottleneck, [3,8,36,3])
```

## （2）DenseNet

**DenseNet：就是在保证网络中层与层之间最大程度的信息传输的前提下，直接把所有层连接起来。换句话说，每一层的输入来自前面所有层的输出**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc8e389ec3a794323a33239a38cd01128.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

```python
import torch
import torch.nn as nn

class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList([nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)
        ) for _ in range(num_layers)])
        
    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)
        return x
    
class TransitionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransitionLayer, self).__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
        
    def forward(self, x):
        return self.transition(x)
    
class DenseNet(nn.Module):
    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),
                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):
        super(DenseNet, self).__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(num_init_features),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = DenseBlock(num_features, growth_rate, num_layers)
            self.features.add_module(f'denseblock_{
              
              i+1}', block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = TransitionLayer(num_features, num_features // 2)
                self.features.add_module(f'transition_{
              
              i+1}', trans)
                num_features = num_features // 2
        
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))
        
        self.classifier = nn.Linear(num_features, num_classes)
        
    def forward(self, x): 
	    features = self.features(x) 
	    out = nn.functional.adaptive_avg_pool2d(features, (1, 1)) 
	    out = out.view(out.size(0), -1) 
	    out = self.classifier(out) 
	    return out

```

## （3）InceptionNet v1-v4

- 仅做了解

# 三：轻量型网络结构

## （1）MobileNet

**MobileNet：由Google团队提出，主要目的是为了设计能够用于移动端的网络结构，使用深度可分离卷积方式代替传统卷积方式。如下图，Depthwise convolution和标准卷积不同，对于标准卷积其卷积核是用在所有的输入通道上（input channels），而depthwise convolution针对每个输入通道采用不同的卷积核，就是说一个卷积核对应一个输入通道，所以说depthwise convolution是depth级别的操作。而pointwise convolution其实就是普通的卷积，只不过其采用1x1的卷积核**

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ff29f529c13e14e06ab582528b726b08e.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

对于depthwise separable convolution，其首先是采用depthwise convolution对不同输入通道分别进行卷积，然后采用pointwise convolution将上面的输出再进行结合，这样其实整体效果和一个标准卷积是差不多的，但是会大大减少计算量和模型参数量。  
![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fca0776b593914c508a7bc6e301f7563d.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

MobileNet网络结构如下

![在这里插入图片描述](https://ziquyun.com/main/csdn/img?url=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fd963a571a1614de7b6000bdb49a20bea.png&rfUrl=https%3A%2F%2Fzhangxing-tech.blog.csdn.net%2Farticle%2Fdetails%2F128921438)

## （2）SquuezeNet

- [知乎：SqueezeNet详解](https://zhuanlan.zhihu.com/p/49465950)

## （3）ShuffleNet

- 知乎：[『高性能模型』轻量级网络ShuffleNet\_v1及v2](https://www.cnblogs.com/hellcat/p/10318630.html)